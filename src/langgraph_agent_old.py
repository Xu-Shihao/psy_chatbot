"""
åŸºäºLangGraphçš„SCID-5é—®è¯Šä»£ç†
å®ç°ç»“æ„åŒ–çš„ç²¾ç¥ç–¾ç—…é—®è¯Šæµç¨‹
"""

from typing import Dict, List, Optional, TypedDict, Annotated
import json
from datetime import datetime
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage, AIMessage
from langgraph.graph import StateGraph, END
from langgraph.graph.message import AnyMessage, add_messages

from config import config
from scid5_knowledge import scid5_kb, Question

class InterviewState(TypedDict):
    """
    é—®è¯ŠçŠ¶æ€ - åŒ…å«å®Œæ•´çš„ä¼šè¯çŠ¶æ€ä¿¡æ¯
    
    åŸºæœ¬çŠ¶æ€å­—æ®µï¼š
    - messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
    - current_question_id: å½“å‰é—®é¢˜ID
    - user_responses: ç”¨æˆ·å›ç­”è®°å½•
    - assessment_complete: è¯„ä¼°æ˜¯å¦å®Œæˆ
    - emergency_situation: æ˜¯å¦æœ‰ç´§æ€¥æƒ…å†µ
    - summary: è¯„ä¼°æ€»ç»“
    - needs_followup: æ˜¯å¦éœ€è¦è¿½é—®
    - conversation_history: å¯¹è¯å†å²è®°å½•
    - chief_complaint: ç”¨æˆ·ä¸»è¦è¯‰æ±‚
    - conversation_mode: å¯¹è¯æ¨¡å¼ ('idle', 'interview', 'chat', 'assessment_complete')
    - chat_therapist_active: CBTç–—æ„ˆå¸ˆæ˜¯å¦æ¿€æ´»
    - mode_detection_result: æ¨¡å¼æ£€æµ‹ç»“æœ
    - conversation_turn_count: ä¼šè¯è½®æ•°è®¡æ•°å™¨
    - interview_mode_locked: é—®è¯Šæ¨¡å¼æ˜¯å¦å·²é”å®š
    
    è¿½é—®å’Œåˆ†æå­—æ®µï¼š
    - followup_questions: è¿½é—®é—®é¢˜åˆ—è¡¨
    - risk_level: é£é™©ç­‰çº§ (low/medium/high)
    - risk_indicators: é£é™©æŒ‡æ ‡åˆ—è¡¨
    - emotional_state: ç”¨æˆ·æƒ…æ„ŸçŠ¶æ€æè¿°
    - content_complete: ç”¨æˆ·å›ç­”æ˜¯å¦å®Œæ•´
    - understanding_summary: å¯¹ç”¨æˆ·å›ç­”çš„ç†è§£æ€»ç»“
    - empathetic_response: å…±æƒ…å›åº”å†…å®¹
    - current_analysis: å½“å‰å®Œæ•´åˆ†æç»“æœ
    
    ä¼šè¯è¿½è¸ªå­—æ®µï¼š
    - debug_info: Debugåˆ†æä¿¡æ¯åˆ—è¡¨
    - session_start_time: ä¼šè¯å¼€å§‹æ—¶é—´
    - question_sequence: é—®é¢˜åºåˆ—è®°å½•
    - final_response: æœ€ç»ˆå›å¤å†…å®¹
    - assessment_duration: è¯„ä¼°æŒç»­æ—¶é—´
    
    ç”¨æˆ·ç‰¹å¾å­—æ®µï¼š
    - user_engagement_level: ç”¨æˆ·å‚ä¸åº¦ (high/medium/low)
    - response_detail_level: å›ç­”è¯¦ç»†ç¨‹åº¦ (detailed/moderate/brief)
    
    ä¸´åºŠè¯„ä¼°å­—æ®µï¼š
    - symptoms_identified: è¯†åˆ«å‡ºçš„ç—‡çŠ¶åˆ—è¡¨
    - domains_assessed: å·²è¯„ä¼°çš„é¢†åŸŸåˆ—è¡¨
    - severity_indicators: å„é¢†åŸŸä¸¥é‡ç¨‹åº¦æŒ‡æ ‡
    """
    messages: Annotated[List[AnyMessage], add_messages]
    current_question_id: Optional[str]
    user_responses: Dict[str, str]
    assessment_complete: bool
    emergency_situation: bool
    summary: str
    needs_followup: bool
    conversation_history: List[str]
    chief_complaint: str  # ç”¨æˆ·çš„ä¸»è¯‰
    
    # å¯¹è¯æ¨¡å¼ç›¸å…³å­—æ®µ
    conversation_mode: str  # å¯¹è¯æ¨¡å¼ï¼š'idle', 'interview', 'chat', 'assessment_complete'
    chat_therapist_active: bool  # CBTç–—æ„ˆå¸ˆæ˜¯å¦æ¿€æ´»
    mode_detection_result: Dict  # æ¨¡å¼æ£€æµ‹ç»“æœå’Œåˆ†æ
    conversation_turn_count: int  # ä¼šè¯è½®æ•°è®¡æ•°å™¨
    interview_mode_locked: bool  # é—®è¯Šæ¨¡å¼æ˜¯å¦å·²é”å®š
    
    # è¿½é—®ç›¸å…³å­—æ®µ
    followup_questions: List[str]  # è¿½é—®é—®é¢˜åˆ—è¡¨
    
    # é£é™©è¯„ä¼°å­—æ®µ
    risk_level: str  # é£é™©ç­‰çº§ï¼šlow/medium/high
    risk_indicators: List[str]  # é£é™©æŒ‡æ ‡åˆ—è¡¨
    
    # æƒ…æ„Ÿå’Œç†è§£åˆ†æå­—æ®µ
    emotional_state: str  # ç”¨æˆ·çš„æƒ…æ„ŸçŠ¶æ€æè¿°
    content_complete: bool  # ç”¨æˆ·å›ç­”æ˜¯å¦å®Œæ•´
    understanding_summary: str  # å¯¹ç”¨æˆ·å›ç­”çš„ç†è§£æ€»ç»“
    empathetic_response: str  # å…±æƒ…å›åº”å†…å®¹
    
    # å½“å‰åˆ†æç»“æœ
    current_analysis: Dict  # å­˜å‚¨æœ€è¿‘ä¸€æ¬¡çš„å®Œæ•´åˆ†æç»“æœ
    
    # Debugå’Œè¿½è¸ªä¿¡æ¯
    debug_info: List[Dict]  # Debugåˆ†æä¿¡æ¯åˆ—è¡¨
    session_start_time: str  # ä¼šè¯å¼€å§‹æ—¶é—´
    question_sequence: List[str]  # é—®é¢˜åºåˆ—è®°å½•
    
    # æœ€ç»ˆè¯„ä¼°ç›¸å…³
    final_response: str  # æœ€ç»ˆå›å¤å†…å®¹
    assessment_duration: int  # è¯„ä¼°æŒç»­æ—¶é—´ï¼ˆåˆ†é’Ÿï¼‰
    
    # ç”¨æˆ·ç‰¹å¾æ ‡è®°
    user_engagement_level: str  # ç”¨æˆ·å‚ä¸åº¦ï¼šhigh/medium/low
    response_detail_level: str  # å›ç­”è¯¦ç»†ç¨‹åº¦ï¼šdetailed/moderate/brief
    
    # ä¸´åºŠç›¸å…³æ ‡è®°
    symptoms_identified: List[str]  # è¯†åˆ«å‡ºçš„ç—‡çŠ¶åˆ—è¡¨
    domains_assessed: List[str]  # å·²è¯„ä¼°çš„é¢†åŸŸåˆ—è¡¨
    severity_indicators: Dict[str, str]  # å„é¢†åŸŸä¸¥é‡ç¨‹åº¦æŒ‡æ ‡
    
    # æ–°å¢çŠ¶æ€å­—æ®µ
    current_topic: str  # å½“å‰è¯é¢˜
    is_follow_up: bool  # æ˜¯å¦æ˜¯è¿½é—®çŠ¶æ€

class SCID5Agent:
    """SCID-5é—®è¯Šä»£ç†"""
    
    def __init__(self, workflow_mode: str = "adaptive"):
        """
        åˆå§‹åŒ–é—®è¯Šä»£ç†
        
        Args:
            workflow_mode: å·¥ä½œæµç¨‹æ¨¡å¼
                - "adaptive": æ™ºèƒ½æ£€æµ‹æ¨¡å¼ï¼Œæ ¹æ®ç”¨æˆ·æ„å›¾è‡ªç”±åˆ‡æ¢é—®è¯Šå’Œé—²èŠ
                - "structured": å›ºå®šæµç¨‹æ¨¡å¼ï¼Œå…ˆå®Œæˆé—®è¯Šå†è½¬CBTé—²èŠ
        """
        self.workflow_mode = workflow_mode
        self.llm = self._initialize_llm()
        self.workflow = self._create_workflow()
        self.app = self.workflow.compile()
    
    def _initialize_llm(self) -> ChatOpenAI:
        """åˆå§‹åŒ–è¯­è¨€æ¨¡å‹"""
        return ChatOpenAI(
            api_key=config.OPENAI_API_KEY,
            base_url=config.OPENAI_API_BASE,
            model=config.MODEL_NAME,
            temperature=config.TEMPERATURE,
            max_tokens=config.MAX_TOKENS
        )
    
    def _create_workflow(self) -> StateGraph:
        """åˆ›å»ºé—®è¯Šå·¥ä½œæµ"""
        workflow = StateGraph(InterviewState)
        
        # æ·»åŠ èŠ‚ç‚¹
        workflow.add_node("start_interview", self.start_interview)
        workflow.add_node("detect_conversation_mode", self.detect_conversation_mode)
        workflow.add_node("chat_therapist_response", self.chat_therapist_response)
        workflow.add_node("understand_and_respond", self.understand_and_respond)
        workflow.add_node("ask_question", self.ask_question)
        workflow.add_node("check_emergency", self.check_emergency)
        workflow.add_node("generate_summary", self.generate_summary)
        workflow.add_node("emergency_response", self.emergency_response)
        
        # è®¾ç½®å…¥å£ç‚¹
        workflow.set_entry_point("start_interview")
        
        # å®šä¹‰è¾¹
        workflow.add_edge("start_interview", "detect_conversation_mode")
        workflow.add_conditional_edges(
            "detect_conversation_mode",
            self.should_continue_after_mode_detection,
            {
                "interview": "ask_question",
                "chat": "chat_therapist_response",
                "continue_interview": "understand_and_respond",
                "assessment_complete": "chat_therapist_response"
            }
        )
        workflow.add_edge("chat_therapist_response", END)
        workflow.add_conditional_edges(
            "ask_question",
            self.should_continue_after_question,
            {
                "wait_response": END,
                "emergency": "emergency_response"
            }
        )
        workflow.add_edge("understand_and_respond", "check_emergency")
        workflow.add_conditional_edges(
            "check_emergency",
            self.should_continue_after_check,
            {
                "continue": "detect_conversation_mode",
                "complete": "generate_summary",
                "emergency": "emergency_response"
            }
        )
        workflow.add_edge("generate_summary", END)
        workflow.add_edge("emergency_response", END)
        
        return workflow
    
    def start_interview(self, state: InterviewState) -> InterviewState:
        """å¼€å§‹é—®è¯Š - ç”Ÿæˆè‡ªæˆ‘ä»‹ç»å’Œå¼•å¯¼å¼€åœºç™½"""
        system_message = SystemMessage(content="""
        ä½ çš„åå­—å«çµæºªæ™ºä¼´ï¼Œæ˜¯ä¸€ä¸ªå¿ƒç†å’¨è¯¢å¸ˆï¼Œèƒ½æ ¹æ®ç”¨æˆ·çš„å›ç­”æ¥è¿›è¡Œé¢„é—®è¯Šå’Œå¿ƒç†ç–å¯¼ã€‚

        è¯·éµå¾ªä»¥ä¸‹åŸåˆ™ï¼š
        1. ä¿æŒä¸“ä¸šã€åŒç†å¿ƒå’Œéæ‰¹åˆ¤çš„æ€åº¦
        2. æ ¹æ®ç”¨æˆ·çš„æƒ…æ„ŸçŠ¶æ€æä¾›è¶³å¤Ÿçš„å…±æƒ…å›åº”
        3. åŸºäºç”¨æˆ·å›ç­”è¿›è¡Œé€‚å½“çš„æ·±å…¥è¯¢é—®
        4. ä¿æŒå¯¹è¯çš„è‡ªç„¶å¯¹è¯çš„æµç•…æ€§ï¼Œä¸è¦è¿‡äºæœºæ¢°åŒ–çš„æé—®ï¼Œè¦åƒä¸€ä¸ªå¿ƒç†å’¨è¯¢å¸ˆä¸€æ ·ä¸ç”¨æˆ·å¯¹è¯
        5. å¦‚å‘ç°ç´§æ€¥æƒ…å†µï¼ˆå¦‚è‡ªæ€é£é™©ï¼‰ï¼Œç«‹å³æä¾›å±æœºå¹²é¢„ä¿¡æ¯
        6. æé†’è¿™æ˜¯ç­›æŸ¥å·¥å…·ï¼Œä¸èƒ½æ›¿ä»£ä¸“ä¸šè¯Šæ–­

        å¯¹è¯é£æ ¼è¦æ±‚ï¼š
        - æ¸©æš–ã€ç†è§£ã€éæ‰¹åˆ¤
        - é€‚æ—¶è¡¨è¾¾å…³åˆ‡å’Œç†è§£
        - é¼“åŠ±ç”¨æˆ·åˆ†äº«æ›´å¤šç»†èŠ‚
        - æ ¹æ®ç”¨æˆ·çš„æƒ…ç»ªçŠ¶æ€è°ƒæ•´è¯­æ°”
        - ä»¥çµæºªæ™ºä¼´çš„èº«ä»½ä¸ç”¨æˆ·å»ºç«‹ä¸“ä¸šè€Œæ¸©æš–çš„å…³ç³»
        """)
        
        # ç”Ÿæˆè‡ªæˆ‘ä»‹ç»å’Œå¼•å¯¼å¼€åœºç™½
        try:
            initial_prompt = """
ä½ çš„åå­—å«çµæºªæ™ºä¼´ï¼Œæ˜¯ä¸€ä½æ¸©æš–ã€ä¸“ä¸šçš„å¿ƒç†å’¨è¯¢å¸ˆï¼Œæ­£åœ¨ä¸ä¸€ä½æ–°çš„æ¥è®¿è€…å¼€å§‹ç¬¬ä¸€æ¬¡ä¼šé¢ã€‚

è¯·ä»¥çµæºªæ™ºä¼´çš„èº«ä»½ç”Ÿæˆä¸€ä¸ªè‡ªæˆ‘ä»‹ç»å’Œå¼€åœºç™½ï¼Œè¦æ±‚ï¼š
1. æ¸©æš–åœ°ä»‹ç»è‡ªå·±çš„å§“åå’Œèº«ä»½ï¼ˆå¿ƒç†å’¨è¯¢å¸ˆï¼‰
2. è¡¨è¾¾å…³å¿ƒå’Œæ¬¢è¿æ¥è®¿è€…çš„æ„Ÿå—
3. è¥é€ å®‰å…¨ã€æ— æ‰¹åˆ¤çš„å’¨è¯¢ç¯å¢ƒæ°›å›´
4. å¼•å¯¼ç”¨æˆ·åˆ†äº«ä»–ä»¬ç›®å‰é‡åˆ°çš„é—®é¢˜æˆ–æƒ³è¦è°ˆè®ºçš„äº‹æƒ…
5. è¯¢é—®ç”¨æˆ·çš„ä¸»è¯‰ï¼ˆæœ€ä¸»è¦çš„å›°æ‰°æˆ–å¸Œæœ›è§£å†³çš„é—®é¢˜ï¼‰
6. é¼“åŠ±ç”¨æˆ·çœŸå®ã€è¯¦ç»†åœ°è¡¨è¾¾è‡ªå·±çš„æ„Ÿå—å’Œç»å†
7. è¯­æ°”è¦ä¸“ä¸šè€Œæ¸©æš–ï¼Œåƒä¸€ä½æœ‰ç»éªŒçš„å¿ƒç†å’¨è¯¢å¸ˆ
8. è®©ç”¨æˆ·æ„Ÿåˆ°è¢«ç†è§£å’Œè¢«æ¥çº³

è¯·ç›´æ¥ç»™å‡ºè‡ªæˆ‘ä»‹ç»å’Œå¼€åœºç™½ï¼Œæ§åˆ¶åœ¨250å­—ä»¥å†…ã€‚
"""
            
            print("=" * 50)
            print("ğŸ” DEBUG - START_INTERVIEW LLM CALL")
            print("PROMPT:")
            print(initial_prompt)
            print("=" * 50)
            
            response = self.llm.invoke([HumanMessage(content=initial_prompt)])
            initial_response = response.content.strip()
            
            print("RESPONSE:")
            print(initial_response)
            print("=" * 50)
            
            # æ·»åŠ é‡è¦è¯´æ˜
            initial_response += """

**é‡è¦è¯´æ˜ï¼š**
- è¿™é‡Œæ˜¯ä¸€ä¸ªå®‰å…¨ã€ä¿å¯†çš„ç©ºé—´ï¼Œè¯·æ”¾å¿ƒåˆ†äº«æ‚¨çš„çœŸå®æ„Ÿå—
- æ²¡æœ‰å¯¹é”™çš„ç­”æ¡ˆï¼Œæˆ‘åœ¨è¿™é‡Œæ˜¯ä¸ºäº†ç†è§£å’Œå¸®åŠ©æ‚¨
- è¿™æ˜¯ä¸€ä¸ªå¿ƒç†å¥åº·ç­›æŸ¥å·¥å…·ï¼Œä¸èƒ½æ›¿ä»£ä¸“ä¸šåŒ»ç–—è¯Šæ–­  
- å¦‚æœ‰ä»»ä½•ç´§æ€¥æƒ…å†µï¼Œè¯·ç«‹å³å¯»æ±‚ä¸“ä¸šå¸®åŠ©
- æˆ‘ä»¬çš„å¯¹è¯å®Œå…¨ä¿å¯†"""
            
        except Exception as e:
            print(f"ğŸ” DEBUG: LLMç”Ÿæˆå¼€å¤´å¤±è´¥: {e}")
            # å¦‚æœLLMè°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨å¼€å¤´
            initial_response = """æ‚¨å¥½ï¼æˆ‘æ˜¯çµæºªæ™ºä¼´ï¼Œä¸€ä½å¿ƒç†å’¨è¯¢å¸ˆã€‚å¾ˆé«˜å…´åœ¨è¿™é‡Œä¸æ‚¨ç›¸é‡ ğŸ˜Š

é¦–å…ˆï¼Œæˆ‘æƒ³è®©æ‚¨çŸ¥é“è¿™é‡Œæ˜¯ä¸€ä¸ªå®‰å…¨ã€æ— æ‰¹åˆ¤çš„ç©ºé—´ã€‚æˆ‘åœ¨è¿™é‡Œæ˜¯ä¸ºäº†å€¾å¬å’Œç†è§£æ‚¨ï¼Œé™ªä¼´æ‚¨ä¸€èµ·é¢å¯¹æ‚¨æ­£åœ¨ç»å†çš„å›°æ‰°ã€‚

æˆ‘æƒ³è¯·æ‚¨è·Ÿæˆ‘åˆ†äº«ä¸€ä¸‹ï¼Œæ˜¯ä»€ä¹ˆè®©æ‚¨ä»Šå¤©æ¥åˆ°è¿™é‡Œï¼Ÿæ‚¨ç›®å‰æœ€ä¸»è¦çš„å›°æ‰°æˆ–å¸Œæœ›è§£å†³çš„é—®é¢˜æ˜¯ä»€ä¹ˆï¼Ÿè¯·å°½å¯èƒ½è¯¦ç»†åœ°å‘Šè¯‰æˆ‘æ‚¨çš„æ„Ÿå—å’Œç»å†ï¼Œä¸ç”¨æ‹…å¿ƒè¯´å¾—ä¸å¤Ÿå¥½æˆ–ä¸å¤Ÿå‡†ç¡®ã€‚

**é‡è¦è¯´æ˜ï¼š**
- è¿™é‡Œæ˜¯ä¸€ä¸ªå®‰å…¨ã€ä¿å¯†çš„ç©ºé—´ï¼Œè¯·æ”¾å¿ƒåˆ†äº«æ‚¨çš„çœŸå®æ„Ÿå—
- æ²¡æœ‰å¯¹é”™çš„ç­”æ¡ˆï¼Œæˆ‘åœ¨è¿™é‡Œæ˜¯ä¸ºäº†ç†è§£å’Œå¸®åŠ©æ‚¨
- è¿™æ˜¯ä¸€ä¸ªå¿ƒç†å¥åº·ç­›æŸ¥å·¥å…·ï¼Œä¸èƒ½æ›¿ä»£ä¸“ä¸šåŒ»ç–—è¯Šæ–­
- å¦‚æœ‰ä»»ä½•ç´§æ€¥æƒ…å†µï¼Œè¯·ç«‹å³å¯»æ±‚ä¸“ä¸šå¸®åŠ©
- æˆ‘ä»¬çš„å¯¹è¯å®Œå…¨ä¿å¯†"""
        
        # åˆ›å»ºAIæ¶ˆæ¯
        intro_message = AIMessage(content=initial_response)
        
        return {
            **state,
            "messages": [system_message, intro_message],
            "current_question_id": "initial",  # è®¾ç½®ä¸ºåˆå§‹çŠ¶æ€ï¼Œç­‰å¾…ç”¨æˆ·åˆ†äº«ä¸»è¯‰
            "user_responses": {},
            "assessment_complete": False,
            "emergency_situation": False,
            "summary": "",
            "needs_followup": False,
            "conversation_history": [],
            "followup_questions": [],
            "risk_level": "low",
            "risk_indicators": [],
            "emotional_state": "",
            "content_complete": False,
            "understanding_summary": "",
            "empathetic_response": "",
            "current_analysis": {},
            "debug_info": [],
            "session_start_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "question_sequence": ["initial"],
            "final_response": "",
            "assessment_duration": 0,
            "user_engagement_level": "medium",
            "response_detail_level": "moderate",
            "symptoms_identified": [],
            "domains_assessed": ["initial"],
            "severity_indicators": {},
            "chief_complaint": "",
            "conversation_mode": "idle",  # å¼€å§‹æ—¶ä¸ºç©ºé—²æ¨¡å¼ï¼Œç­‰å¾…ç”¨æˆ·æ„å›¾æ£€æµ‹
            "chat_therapist_active": False,
            "mode_detection_result": {},
            "conversation_turn_count": 0,  # åˆå§‹åŒ–ä¼šè¯è½®æ•°
            "interview_mode_locked": False,  # åˆå§‹åŒ–é—®è¯Šæ¨¡å¼é”å®šçŠ¶æ€
            "current_topic": "åˆå§‹åŒ–é—®è¯Š",
            "is_follow_up": False
        }
    
    def detect_conversation_mode(self, state: InterviewState) -> InterviewState:
        """æ£€æµ‹å¯¹è¯æ¨¡å¼ - åœ¨å‰5è½®æ£€æµ‹ç”¨æˆ·æ„å›¾ï¼Œä¸€æ—¦è¿›å…¥é—®è¯Šæ¨¡å¼å°±é”å®š"""
        # å¦‚æœå·²ç»å®Œæˆè¯„ä¼°ï¼Œé»˜è®¤è¿›å…¥é—²èŠæ¨¡å¼
        if state.get("assessment_complete", False):
            return {
                **state,
                "conversation_mode": "assessment_complete",
                "chat_therapist_active": True,
                "mode_detection_result": {
                    "detected_mode": "assessment_complete",
                    "confidence": 1.0,
                    "reason": "è¯„ä¼°å·²å®Œæˆï¼Œè¿›å…¥åç»­é—²èŠæ¨¡å¼"
                }
            }
        
        # è·å–å½“å‰ä¼šè¯è½®æ•°
        current_turn = state.get("conversation_turn_count", 0)
        interview_locked = state.get("interview_mode_locked", False)
        
        # å¦‚æœé—®è¯Šæ¨¡å¼å·²é”å®šï¼Œå¼ºåˆ¶ç»§ç»­é—®è¯Š
        if interview_locked:
            return {
                **state,
                "conversation_mode": "continue_interview",
                "chat_therapist_active": False,
                "conversation_turn_count": current_turn + 1,
                "mode_detection_result": {
                    "detected_mode": "continue_interview",
                    "confidence": 1.0,
                    "reason": "é—®è¯Šæ¨¡å¼å·²é”å®šï¼Œç»§ç»­è¿›è¡Œé—®è¯Šç›´è‡³å®Œæˆ"
                }
            }
        
        # ç»“æ„åŒ–æ¨¡å¼ï¼šå¦‚æœè¯„ä¼°æœªå®Œæˆï¼Œå¼ºåˆ¶è¿›å…¥é—®è¯Šæ¨¡å¼
        if (self.workflow_mode == "structured" and 
            not state.get("assessment_complete", False)):
            return {
                **state,
                "conversation_mode": "continue_interview",
                "chat_therapist_active": False,
                "conversation_turn_count": current_turn + 1,
                "interview_mode_locked": True,  # ç»“æ„åŒ–æ¨¡å¼ç›´æ¥é”å®šé—®è¯Š
                "mode_detection_result": {
                    "detected_mode": "continue_interview",
                    "confidence": 1.0,
                    "reason": "ç»“æ„åŒ–æ¨¡å¼ï¼šå¼ºåˆ¶è¿›å…¥é—®è¯Šæµç¨‹å¹¶é”å®š"
                }
            }
        
        # è·å–æœ€æ–°çš„ç”¨æˆ·æ¶ˆæ¯
        user_messages = [msg for msg in state["messages"] if isinstance(msg, HumanMessage)]
        if not user_messages:
            return {
                **state,
                "conversation_mode": "interview",
                "conversation_turn_count": current_turn + 1,
                "mode_detection_result": {
                    "detected_mode": "interview",
                    "confidence": 0.8,
                    "reason": "æ²¡æœ‰ç”¨æˆ·è¾“å…¥ï¼Œé»˜è®¤å¼€å§‹é—®è¯Š"
                }
            }
        
        latest_user_message = user_messages[-1].content
        
        # å¦‚æœè¶…è¿‡5è½®ä¸”æœªè¿›å…¥é—®è¯Šæ¨¡å¼ï¼Œè‡ªåŠ¨é”å®šä¸ºé—®è¯Šæ¨¡å¼
        if current_turn >= 5 and state.get("conversation_mode", "idle") != "interview":
            return {
                **state,
                "conversation_mode": "interview",
                "chat_therapist_active": False,
                "conversation_turn_count": current_turn + 1,
                "interview_mode_locked": True,
                "mode_detection_result": {
                    "detected_mode": "interview",
                    "confidence": 1.0,
                    "reason": "å·²è¾¾5è½®å¯¹è¯ï¼Œè‡ªåŠ¨é”å®šä¸ºé—®è¯Šæ¨¡å¼"
                }
            }
        
        # å‰5è½®è¿›è¡Œæ™ºèƒ½æ„å›¾æ£€æµ‹
        if current_turn < 5:
            # ä½¿ç”¨LLMè¿›è¡Œæ¨¡å¼æ£€æµ‹
            detection_prompt = f"""
            è¯·åˆ†æç”¨æˆ·çš„è¾“å…¥ï¼Œåˆ¤æ–­ç”¨æˆ·æ˜¯æƒ³è¦è¿›è¡Œå¿ƒç†å¥åº·é—®è¯Šè¿˜æ˜¯æƒ³è¦é—²èŠï¼š

            ç”¨æˆ·è¾“å…¥ï¼š"{latest_user_message}"
            å½“å‰æ˜¯ç¬¬{current_turn + 1}è½®å¯¹è¯

            åˆ¤æ–­æ ‡å‡†ï¼š
            1. å¦‚æœç”¨æˆ·æ˜ç¡®è¡¨è¾¾æƒ³è¦é—²èŠã€èŠå¤©ã€è°ˆå¿ƒç­‰ï¼Œåˆ™ä¸º"chat"æ¨¡å¼
            2. å¦‚æœç”¨æˆ·æåˆ°äº†å…·ä½“çš„ç²¾ç¥å¥åº·ç—‡çŠ¶ã€å¿ƒç†é—®é¢˜ã€æƒ…ç»ªå›°æ‰°ç­‰ï¼Œåˆ™ä¸º"interview"æ¨¡å¼
            3. å¦‚æœç”¨æˆ·æ­£åœ¨å›ç­”é—®è¯Šé—®é¢˜ï¼Œåˆ™ä¸º"continue_interview"æ¨¡å¼
            4. å¦‚æœç”¨æˆ·è¯´"å¼€å§‹è¯„ä¼°"æˆ–ç±»ä¼¼çš„è¡¨è¾¾ï¼Œåˆ™ä¸º"interview"æ¨¡å¼

            è¯·ä»¥JSONæ ¼å¼å›å¤ï¼š
            {{
                "mode": "chat/interview/continue_interview",
                "confidence": 0.0-1.0,
                "reason": "åˆ¤æ–­ç†ç”±",
                "key_indicators": ["å…³é”®æŒ‡æ ‡1", "å…³é”®æŒ‡æ ‡2"]
            }}
            """
            
            try:
                print("=" * 50)
                print("ğŸ” DEBUG - DETECT_CONVERSATION_MODE LLM CALL")
                print("PROMPT:")
                print(detection_prompt)
                print("=" * 50)
                
                detection_response = self.llm.invoke([
                    SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å¿ƒç†å¥åº·å¯¹è¯åˆ†æå¸ˆï¼Œèƒ½å¤Ÿå‡†ç¡®åˆ¤æ–­ç”¨æˆ·çš„å¯¹è¯æ„å›¾ã€‚"),
                    HumanMessage(content=detection_prompt)
                ])
                
                print("RESPONSE:")
                print(detection_response.content)
                print("=" * 50)
                
                detection_result = json.loads(detection_response.content)
                detected_mode = detection_result["mode"]
                
                # å¦‚æœæ£€æµ‹åˆ°é—®è¯Šæ¨¡å¼ï¼Œç«‹å³é”å®š
                interview_will_lock = detected_mode in ["interview", "continue_interview"]
                
                # æ ¹æ®å½“å‰çŠ¶æ€è°ƒæ•´æ£€æµ‹ç»“æœ
                current_mode = state.get("conversation_mode", "idle")
                if current_mode == "interview" and detected_mode == "interview":
                    detected_mode = "continue_interview"
                
                return {
                    **state,
                    "conversation_mode": detected_mode,
                    "chat_therapist_active": detected_mode == "chat",
                    "conversation_turn_count": current_turn + 1,
                    "interview_mode_locked": interview_will_lock,
                    "mode_detection_result": {
                        **detection_result,
                        "turn_count": current_turn + 1,
                        "locked": interview_will_lock
                    }
                }
                
            except Exception as e:
                print(f"æ¨¡å¼æ£€æµ‹å¤±è´¥: {e}")
                # åå¤‡é€»è¾‘ï¼šç®€å•å…³é”®è¯æ£€æµ‹
                chat_keywords = ["é—²èŠ", "èŠå¤©", "è°ˆå¿ƒ", "èŠèŠ", "éšä¾¿èŠ", "é™ªæˆ‘èŠ"]
                interview_keywords = ["æŠ‘éƒ", "ç„¦è™‘", "å¤±çœ ", "æƒ…ç»ª", "å¿ƒç†", "ç—‡çŠ¶", "å›°æ‰°", "é—®é¢˜"]
                
                if any(keyword in latest_user_message for keyword in chat_keywords):
                    mode = "chat"
                    will_lock = False
                elif any(keyword in latest_user_message for keyword in interview_keywords):
                    mode = "interview"
                    will_lock = True
                else:
                    mode = "continue_interview" if state.get("conversation_mode") == "interview" else "interview"
                    will_lock = True
                
                return {
                    **state,
                    "conversation_mode": mode,
                    "chat_therapist_active": mode == "chat",
                    "conversation_turn_count": current_turn + 1,
                    "interview_mode_locked": will_lock,
                    "mode_detection_result": {
                        "detected_mode": mode,
                        "confidence": 0.6,
                        "reason": "ä½¿ç”¨å…³é”®è¯æ£€æµ‹åå¤‡é€»è¾‘",
                        "key_indicators": [],
                        "turn_count": current_turn + 1,
                        "locked": will_lock
                    }
                }
        
        # è¶…è¿‡5è½®çš„æƒ…å†µï¼ˆç†è®ºä¸Šä¸åº”è¯¥åˆ°è¾¾è¿™é‡Œï¼Œå› ä¸ºä¸Šé¢å·²ç»å¤„ç†äº†ï¼‰
        return {
            **state,
            "conversation_mode": "interview",
            "chat_therapist_active": False,
            "conversation_turn_count": current_turn + 1,
            "interview_mode_locked": True,
            "mode_detection_result": {
                "detected_mode": "interview",
                "confidence": 1.0,
                "reason": "è¶…è¿‡5è½®å¯¹è¯ï¼Œå¼ºåˆ¶é”å®šä¸ºé—®è¯Šæ¨¡å¼"
            }
        }
    
    def chat_therapist_response(self, state: InterviewState) -> InterviewState:
        """CBTç–—æ„ˆå¸ˆå“åº” - å½“ç”¨æˆ·æƒ³è¦é—²èŠæ—¶æä¾›å¿ƒç†æ”¯æŒ"""
        user_messages = [msg for msg in state["messages"] if isinstance(msg, HumanMessage)]
        latest_user_message = user_messages[-1].content if user_messages else ""
        
        # æ„å»ºCBTç–—æ„ˆå¸ˆçš„ç³»ç»Ÿæç¤º
        cbt_system_prompt = """ä½ æ˜¯çµæºªæ™ºä¼´ï¼Œä¸€ä½ä¸“ä¸šçš„è®¤çŸ¥è¡Œä¸ºç–—æ³•(CBT)å¿ƒç†ç–—æ„ˆå¸ˆã€‚
        
ä½ çš„ä»»åŠ¡æ˜¯ï¼š
1. ç”Ÿæˆæ¸©æš–ã€ç†è§£å’Œæ”¯æŒæ€§çš„å¯¹è¯
2. è¿ç”¨CBTæŠ€å·§å¸®åŠ©ç”¨æˆ·è®¤è¯†å’Œæ”¹å˜æ¶ˆææ€ç»´æ¨¡å¼
3. å¼•å¯¼ç”¨æˆ·æ¢ç´¢æƒ…æ„Ÿå’Œè¡Œä¸ºæ¨¡å¼
4. æä¾›å®ç”¨çš„åº”å¯¹ç­–ç•¥å’ŒæŠ€å·§
5. åˆ›é€ å®‰å…¨ã€éæ‰¹åˆ¤çš„å¯¹è¯ç¯å¢ƒ

å¯¹è¯é£æ ¼ï¼š
- æ¸©æš–ã€å…±æƒ…ã€ä¸“ä¸š
- é—®å¼€æ”¾æ€§é—®é¢˜å¼•å¯¼æ€è€ƒ
- é€‚æ—¶æä¾›CBTæŠ€å·§å’Œç­–ç•¥
- é¼“åŠ±ç”¨æˆ·è¡¨è¾¾å†…å¿ƒæ„Ÿå—
- å¸®åŠ©ç”¨æˆ·å»ºç«‹ç§¯æçš„è®¤çŸ¥æ¨¡å¼

è¯·æ³¨æ„ï¼š
- è¿™æ˜¯æ”¯æŒæ€§é—²èŠï¼Œä¸æ˜¯æ­£å¼çš„é—®è¯Š
- å¦‚æœç”¨æˆ·æåˆ°ä¸¥é‡çš„å¿ƒç†å¥åº·é—®é¢˜ï¼Œå»ºè®®å¯»æ±‚ä¸“ä¸šå¸®åŠ©
- ä¿æŒå¯¹è¯çš„è½»æ¾å’Œæ”¯æŒæ€§
- ä¸ç”¨ç”Ÿæˆå¼€å¤´çš„æ„Ÿæƒ…è¡¨æƒ…çš„æ ‡ç­¾ï¼Œç›´æ¥ç”Ÿæˆå¯¹è¯å†…å®¹"""
        
        # å¦‚æœæ˜¯è¯„ä¼°å®Œæˆåçš„é—²èŠ
        if state.get("conversation_mode") == "assessment_complete":
            cbt_system_prompt += """
            
            ç‰¹åˆ«æé†’ï¼šç”¨æˆ·åˆšåˆšå®Œæˆäº†å¿ƒç†å¥åº·è¯„ä¼°ï¼Œç°åœ¨è¿›å…¥é—²èŠç¯èŠ‚ã€‚è¯·ï¼š
            1. æ„Ÿè°¢ç”¨æˆ·çš„å‚ä¸å’Œé…åˆ
            2. ç¡®è®¤è¯„ä¼°å·²å®Œæˆ
            3. æä¾›åç»­çš„å¿ƒç†æ”¯æŒå’Œå»ºè®®
            4. è¯¢é—®ç”¨æˆ·æ˜¯å¦æœ‰å…¶ä»–æƒ³èŠçš„è¯é¢˜
            """
        
        try:
            # ç”ŸæˆCBTç–—æ„ˆå¸ˆçš„å›å¤
            cbt_prompt = f"ç”¨æˆ·è¯´ï¼š{latest_user_message}"
            
            print("=" * 50)
            print("ğŸ” DEBUG - CBT_THERAPIST_RESPONSE LLM CALL")
            print("SYSTEM PROMPT:")
            print(cbt_system_prompt)
            print("USER PROMPT:")
            print(cbt_prompt)
            print("=" * 50)
            
            cbt_response = self.llm.invoke([
                SystemMessage(content=cbt_system_prompt),
                HumanMessage(content=cbt_prompt)
            ])
            
            print("RESPONSE:")
            print(cbt_response.content)
            print("=" * 50)
            
            final_response = cbt_response.content
            
            # å¦‚æœæ˜¯è¯„ä¼°å®ŒæˆçŠ¶æ€ï¼Œæ·»åŠ æ„Ÿè°¢å’Œç¡®è®¤ä¿¡æ¯
            if state.get("conversation_mode") == "assessment_complete" and "æ„Ÿè°¢" not in final_response:
                completion_message = """æ„Ÿè°¢æ‚¨çš„å‚ä¸å’Œé…åˆï¼æˆ‘å·²ç»å®Œæˆäº†å¯¹æ‚¨çš„å¿ƒç†å¥åº·è¯„ä¼°ã€‚ç°åœ¨æˆ‘ä»¬å¯ä»¥è¿›è¡Œè½»æ¾çš„äº¤æµå’Œå¿ƒç†æ”¯æŒã€‚å¦‚æœæ‚¨æœ‰ä»»ä½•æƒ³èŠçš„è¯é¢˜æˆ–éœ€è¦å¿ƒç†æ”¯æŒï¼Œæˆ‘å¾ˆä¹æ„é™ªä¼´æ‚¨ã€‚
                """
                final_response = completion_message + "\n\n" + final_response
            
            return {
                **state,
                "final_response": final_response,
                "chat_therapist_active": True
            }
            
        except Exception as e:
            print(f"CBTç–—æ„ˆå¸ˆå“åº”ç”Ÿæˆå¤±è´¥: {e}")
            # åå¤‡å“åº”
            fallback_response = """
            ä½œä¸ºæ‚¨çš„å¿ƒç†æ”¯æŒä¼™ä¼´ï¼Œæˆ‘å¾ˆé«˜å…´èƒ½å’Œæ‚¨èŠå¤©ã€‚æ‚¨æƒ³èŠä»€ä¹ˆå‘¢ï¼Ÿ
            
            æˆ‘å¯ä»¥ï¼š
            - å€¾å¬æ‚¨çš„æ„Ÿå—å’Œæƒ³æ³•
            - æä¾›æƒ…ç»ªæ”¯æŒå’Œç†è§£
            - åˆ†äº«ä¸€äº›å¿ƒç†å¥åº·çš„å°æŠ€å·§
            - é™ªæ‚¨æ¢è®¨ç”Ÿæ´»ä¸­çš„å„ç§è¯é¢˜
            
            è¯·å‘Šè¯‰æˆ‘ï¼Œæ‚¨ä»Šå¤©æ„Ÿè§‰å¦‚ä½•ï¼Ÿ
            """
            
            return {
                **state,
                "final_response": fallback_response,
                "chat_therapist_active": True
            }
    
    def understand_and_respond(self, state: InterviewState) -> InterviewState:
        """ç†è§£ç”¨æˆ·å›ç­”å¹¶æä¾›å…±æƒ…å›åº”"""
        if len(state["messages"]) < 2:
            return state
        
        # è·å–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
        last_user_message = None
        for msg in reversed(state["messages"]):
            if isinstance(msg, HumanMessage):
                last_user_message = msg.content
                break
        
        if not last_user_message:
            return state
        
        current_question_id = state["current_question_id"]
        current_question = scid5_kb.questions.get(current_question_id) if current_question_id else None
        
        # æ„å»ºç†è§£å’Œå›åº”çš„æç¤º
        conversation_context = "\n".join(state["conversation_history"][-3:]) if state["conversation_history"] else ""
        
        understanding_prompt = f"""
åŸºäºä»¥ä¸‹å¯¹è¯æƒ…å¢ƒï¼Œç†è§£ç”¨æˆ·çš„å›ç­”å¹¶æä¾›é€‚å½“çš„å›åº”ï¼š

å½“å‰é—®é¢˜èƒŒæ™¯ï¼š{current_question.text if current_question else "æ— "}

æœ€è¿‘å¯¹è¯å†å²ï¼š{conversation_context}

ç”¨æˆ·å›ç­”ï¼š{last_user_message}

è¯·è¿›è¡Œä»¥ä¸‹åˆ†æå’Œå›åº”ï¼š

1. **æƒ…æ„Ÿç†è§£**ï¼šè¯†åˆ«ç”¨æˆ·å›ç­”ä¸­çš„æƒ…æ„ŸçŠ¶æ€ï¼ˆå¦‚ç„¦è™‘ã€æ‚²ä¼¤ã€ææƒ§ã€æ— åŠ©ç­‰ï¼‰

2. **å†…å®¹åˆ†æ**ï¼š
    - ç”¨æˆ·æ˜¯å¦å®Œæ•´å›ç­”äº†é—®é¢˜ï¼Ÿ
    - æ˜¯å¦éœ€è¦è¿›ä¸€æ­¥æ¾„æ¸…æˆ–æ·±å…¥äº†è§£ï¼Ÿ
    - æ˜¯å¦æœ‰é‡è¦çš„ç»†èŠ‚éœ€è¦è¿½é—®ï¼Ÿ

3. **å…±æƒ…å›åº”**ï¼šåŸºäºç”¨æˆ·çš„æƒ…æ„ŸçŠ¶æ€ï¼Œæä¾›æ¸©æš–ã€ç†è§£çš„å›åº”

4. **è¿½é—®å»ºè®®**ï¼šå¦‚æœéœ€è¦ï¼Œæå‡º1-2ä¸ªè‡ªç„¶çš„è¿½é—®é—®é¢˜

5. **é£é™©è¯„ä¼°**ï¼šæ˜¯å¦æ¶‰åŠè‡ªä¼¤ã€è‡ªæ€æˆ–å…¶ä»–ç´§æ€¥æƒ…å†µï¼Ÿ

è¯·ä»¥JSONæ ¼å¼å›å¤ï¼š
{{
    "emotional_state": "ç”¨æˆ·çš„æƒ…æ„ŸçŠ¶æ€æè¿°",
    "content_complete": true/false,
    "empathetic_response": "å…±æƒ…å›åº”å†…å®¹",
    "followup_needed": true/false,
    "followup_questions": ["è¿½é—®é—®é¢˜1", "è¿½é—®é—®é¢˜2"],
    "risk_level": "low/medium/high",
    "understanding_summary": "å¯¹ç”¨æˆ·å›ç­”çš„ç†è§£æ€»ç»“"
}}
        """
        
        print("=" * 50)
        print("ğŸ” DEBUG - UNDERSTAND_AND_RESPOND LLM CALL")
        print("PROMPT:")
        print(understanding_prompt)
        print("=" * 50)
        
        response = self.llm.invoke([HumanMessage(content=understanding_prompt)])
        
        print("RESPONSE:")
        print(response.content)
        print("=" * 50)
        
        try:
            analysis = json.loads(response.content)
        except:
            # å¦‚æœJSONè§£æå¤±è´¥ï¼Œæä¾›é»˜è®¤å›åº”
            analysis = {
                "emotional_state": "æœªèƒ½è¯†åˆ«",
                "content_complete": True,
                "empathetic_response": "è°¢è°¢æ‚¨çš„åˆ†äº«ã€‚",
                "followup_needed": False,
                "followup_questions": [],
                "risk_level": "low",
                "risk_indicators": [],
                "understanding_summary": last_user_message
            }
        
        # è®°å½•ç”¨æˆ·å›ç­”
        updated_responses = state["user_responses"].copy()
        if current_question_id:
            updated_responses[current_question_id] = last_user_message
        
        # æ›´æ–°å¯¹è¯å†å²
        updated_history = state["conversation_history"].copy()
        updated_history.append(f"Q: {current_question.text if current_question else ''}")
        updated_history.append(f"A: {last_user_message}")
        # updated_history.append(f"Analysis: {analysis['understanding_summary']}")
        
        # ç”Ÿæˆå›åº”æ¶ˆæ¯
        response_parts = []
        
        # æ·»åŠ å…±æƒ…å›åº”
        if analysis["empathetic_response"]:
            response_parts.append(analysis["empathetic_response"])
        
        # æ·»åŠ è¿½é—®
        if analysis["followup_needed"] and analysis["followup_questions"]:
            response_parts.append("\næˆ‘æƒ³è¿›ä¸€æ­¥äº†è§£ä¸€ä¸‹ï¼š")
            for i, question in enumerate(analysis["followup_questions"][:2], 1):
                response_parts.append(f"{i}. {question}")
        
        if response_parts:
            ai_response = AIMessage(content="\n".join(response_parts))
            messages_to_add = [ai_response]
        else:
            messages_to_add = []
        
        return {
            **state,
            "messages": state["messages"] + messages_to_add,
            "user_responses": updated_responses,
            "needs_followup": analysis["followup_needed"],
            "conversation_history": updated_history,
            "emergency_situation": analysis["risk_level"] == "high",
            "risk_level": analysis["risk_level"],
            "risk_indicators": analysis.get("risk_indicators", []),
            "emotional_state": analysis["emotional_state"],
            "content_complete": analysis["content_complete"],
            "understanding_summary": analysis["understanding_summary"],
            "empathetic_response": analysis["empathetic_response"],
            "current_analysis": analysis,
            "session_start_time": state["session_start_time"],
            "question_sequence": state["question_sequence"] + [current_question_id] if current_question_id else state["question_sequence"],
            "assessment_duration": state["assessment_duration"] + 1 if current_question_id else state["assessment_duration"],
            "user_engagement_level": "high" if analysis["risk_level"] == "high" else "medium" if analysis["risk_level"] == "medium" else "low",
            "response_detail_level": "detailed" if len(analysis.get("risk_indicators", [])) > 0 else "moderate" if analysis["risk_level"] == "medium" else "brief",
            "symptoms_identified": state["symptoms_identified"] + [current_question.text] if current_question else state["symptoms_identified"],
            "domains_assessed": state["domains_assessed"] + [current_question.category.value] if current_question else state["domains_assessed"],
            "severity_indicators": {**state["severity_indicators"], **{current_question.category.value: analysis["risk_level"]}} if current_question else state["severity_indicators"],
            "chief_complaint": state["chief_complaint"],
            "conversation_mode": state["conversation_mode"],
            "chat_therapist_active": state["chat_therapist_active"],
            "mode_detection_result": state["mode_detection_result"],
            "current_topic": state["current_topic"],
            "is_follow_up": state["is_follow_up"]
        }
    
    def ask_question(self, state: InterviewState) -> InterviewState:
        """æ™ºèƒ½æé—®"""
        current_question_id = state["current_question_id"]
        
        # å¦‚æœéœ€è¦è¿½é—®ï¼Œä¸è¦é—®æ–°é—®é¢˜
        if state.get("needs_followup", False):
            return state
        
        if not current_question_id:
            return {**state, "assessment_complete": True}
        
        current_question = scid5_kb.questions.get(current_question_id)
        if not current_question:
            return {**state, "assessment_complete": True}
        
        # è·å–å¯¹è¯ä¸Šä¸‹æ–‡
        conversation_context = "\n".join(state["conversation_history"][-10:]) if state["conversation_history"] else ""
        
        # ä½¿ç”¨LLMç”Ÿæˆæ›´æ™ºèƒ½ã€æ›´è‡ªç„¶çš„é—®é¢˜
        question_prompt = f"""
åŸºäºä»¥ä¸‹ä¿¡æ¯ï¼Œä»¥æ¸©æš–ã€ä¸“ä¸šçš„æ–¹å¼æå‡ºä¸‹ä¸€ä¸ªé—®é¢˜ï¼š

ç»“æ„åŒ–é—®é¢˜ï¼š{current_question.text}
é—®é¢˜ç›®çš„ï¼š{current_question.purpose if hasattr(current_question, 'purpose') else 'è¯„ä¼°ç›¸å…³ç—‡çŠ¶'}

å¯¹è¯å†å²ï¼š
{conversation_context}

è¯·éµå¾ªä»¥ä¸‹è¦æ±‚ï¼š
1. ä¿æŒé—®é¢˜çš„è¯Šæ–­ç›®çš„ï¼Œä½†ä½¿ç”¨æ›´è‡ªç„¶ã€æ¸©å’Œçš„å£è¯­è¡¨è¾¾æ–¹å¼
2. æ ¹æ®å¯¹è¯å†å²ï¼Œé€‚å½“è°ƒæ•´é—®é¢˜çš„è¡¨è¿°å’Œè¯­æ°”
3. å¦‚æœç”¨æˆ·ä¹‹å‰è¡¨ç°å‡ºæƒ…ç»ªå›°æ‰°ï¼Œåœ¨é—®é¢˜å‰åŠ å…¥é€‚å½“çš„å…³æ€€è¡¨è¾¾
4. ä½¿é—®é¢˜å¬èµ·æ¥åƒè‡ªç„¶å¯¹è¯ï¼Œè€Œéæœºæ¢°åŒ–çš„æ¸…å•æ£€æŸ¥ï¼Œä¸è¦ä½¿ç”¨1.2.3.è¿™æ ·çš„æ ¼å¼
5. å¦‚æœéœ€è¦ï¼Œå¯ä»¥æä¾›ä¸€äº›è§£é‡Šæˆ–èƒŒæ™¯ä¿¡æ¯å¸®åŠ©ç”¨æˆ·ç†è§£
6. å¦‚æœç”¨æˆ·å‰åºå›ç­”ä¸­å·²ç»è¯´æ²¡æœ‰è‡ªæ€æƒ³æ³•ï¼Œä¸è¦æŒç»­è¯¢é—®è‡ªæ€å’Œè‡ªä¼¤çš„é—®é¢˜

ç›´æ¥è¿”å›ä¼˜åŒ–åçš„é—®é¢˜æ–‡æœ¬ï¼Œä¸éœ€è¦å…¶ä»–æ ¼å¼ã€‚
        """
        
        print("=" * 50)
        print("ğŸ” DEBUG - ASK_QUESTION LLM CALL")
        print("PROMPT:")
        print(question_prompt)
        print("=" * 50)
        
        response = self.llm.invoke([HumanMessage(content=question_prompt)])
        
        print("RESPONSE:")
        print(response.content)
        print("=" * 50)
        
        formatted_question = response.content.strip()
        
        # æ·»åŠ é—®é¢˜åˆ°æ¶ˆæ¯ä¸­
        question_message = AIMessage(content=formatted_question)
        
        return {
            **state,
            "messages": state["messages"] + [question_message],
            "needs_followup": False
        }
    
    def check_emergency(self, state: InterviewState) -> InterviewState:
        """æ£€æŸ¥ç´§æ€¥æƒ…å†µ"""
        # è·å–æœ€è¿‘çš„ç”¨æˆ·å›ç­”
        last_response = ""
        for msg in reversed(state["messages"]):
            if isinstance(msg, HumanMessage):
                last_response = msg.content.lower()
                break
        
        # ä½¿ç”¨LLMè¿›è¡Œæ›´æ™ºèƒ½çš„é£é™©è¯„ä¼°
        risk_prompt = f"""è¯·è¯„ä¼°ä»¥ä¸‹ç”¨æˆ·å›ç­”ä¸­çš„è‡ªæ€é£é™©æˆ–ç´§æ€¥å¿ƒç†å¥åº·é£é™©ï¼š

ç”¨æˆ·å›ç­”ï¼š{last_response}
å½“å‰é—®é¢˜IDï¼š{state.get("current_question_id", "")}

è¯„ä¼°æ ‡å‡†ï¼š
1. ç›´æ¥æˆ–é—´æ¥æåˆ°è‡ªæ€æƒ³æ³•
2. è¡¨è¾¾ç»æœ›ã€æ— åŠ©æ„Ÿ
3. æåˆ°è‡ªä¼¤è¡Œä¸º
4. è¡¨è¾¾æƒ³è¦"æ¶ˆå¤±"æˆ–"ç»“æŸä¸€åˆ‡"
5. æåˆ°å…·ä½“çš„è‡ªæ€è®¡åˆ’æˆ–æ–¹æ³•

è¯·ä»¥JSONæ ¼å¼å›å¤ï¼š
{{
    "risk_level": "low/medium/high",
    "risk_indicators": ["è¯†åˆ«åˆ°çš„é£é™©æŒ‡æ ‡åˆ—è¡¨"],
    "immediate_action_needed": true/false,
    "reasoning": "é£é™©è¯„ä¼°ç†ç”±"
}}
        """
        
        print("=" * 50)
        print("ğŸ” DEBUG - CHECK_EMERGENCY LLM CALL")
        print("PROMPT:")
        print(risk_prompt)
        print("=" * 50)
        
        response = self.llm.invoke([HumanMessage(content=risk_prompt)])
        
        print("RESPONSE:")
        print(response.content)
        print("=" * 50)
        
        try:
            risk_analysis = json.loads(response.content)
            emergency_detected = risk_analysis.get("immediate_action_needed", False)
        except:
            # å¦‚æœLLMåˆ†æå¤±è´¥ï¼Œä½¿ç”¨å…³é”®è¯æ£€æµ‹ä½œä¸ºå¤‡é€‰
            emergency_keywords = ["è‡ªæ€", "ç»“æŸç”Ÿå‘½", "ä¸æƒ³æ´»", "æ­»äº†ç®—äº†", "è‡ªæ€è®¡åˆ’"]
            emergency_detected = any(keyword in last_response for keyword in emergency_keywords)
        
        return {
            **state,
            "emergency_situation": emergency_detected
        }
    
    def should_continue_after_mode_detection(self, state: InterviewState) -> str:
        """å†³å®šæ¨¡å¼æ£€æµ‹åçš„ä¸‹ä¸€æ­¥"""
        # å¦‚æœæ˜¯ç»“æ„åŒ–æ¨¡å¼ä¸”æœªå®Œæˆè¯„ä¼°ï¼Œå¼ºåˆ¶è¿›å…¥é—®è¯Šæµç¨‹
        if (self.workflow_mode == "structured" and 
            not state.get("assessment_complete", False)):
            return "interview"
        
        # æ™ºèƒ½æ£€æµ‹æ¨¡å¼çš„åŸæœ‰é€»è¾‘
        mode = state.get("conversation_mode", "interview")
        
        if mode == "chat":
            return "chat"
        elif mode == "interview":
            return "interview"
        elif mode == "continue_interview":
            return "continue_interview"
        elif mode == "assessment_complete":
            return "assessment_complete"
        else:
            return "interview"  # é»˜è®¤è¿›å…¥é—®è¯Šæ¨¡å¼
    
    def should_continue_after_question(self, state: InterviewState) -> str:
        """é—®é¢˜åçš„æµç¨‹æ§åˆ¶"""
        if state["emergency_situation"]:
            return "emergency"
        return "wait_response"
    
    def should_continue_after_check(self, state: InterviewState) -> str:
        """æ£€æŸ¥åçš„æµç¨‹æ§åˆ¶"""
        if state["emergency_situation"]:
            return "emergency"
        elif state["assessment_complete"]:
            return "complete"
        else:
            # è·å–ä¸‹ä¸€ä¸ªé—®é¢˜
            current_question_id = state["current_question_id"]
            last_response = ""
            for msg in reversed(state["messages"]):
                if isinstance(msg, HumanMessage):
                    last_response = msg.content
                    break
            
            if current_question_id and last_response:
                next_question = scid5_kb.get_next_question(current_question_id, last_response)
                if next_question:
                    # æ›´æ–°åˆ°ä¸‹ä¸€ä¸ªé—®é¢˜
                    state["current_question_id"] = next_question.id
                    return "continue"
                else:
                    state["assessment_complete"] = True
                    return "complete"
            
            return "complete"
    
    def generate_summary(self, state: InterviewState) -> InterviewState:
        """ç”Ÿæˆè¯„ä¼°æ€»ç»“"""
        assessment_summary = scid5_kb.generate_assessment_summary()
        
        # ä½¿ç”¨LLMç”Ÿæˆæ›´è¯¦ç»†å’Œä¸ªæ€§åŒ–çš„æ€»ç»“
        prompt = f"""åŸºäºä»¥ä¸‹å®Œæ•´çš„å¿ƒç†å¥åº·ç­›æŸ¥å¯¹è¯ï¼Œç”Ÿæˆä¸€ä»½ä¸“ä¸šã€ä¸ªæ€§åŒ–çš„è¯„ä¼°æ€»ç»“ï¼š

å¯¹è¯å†å²ï¼š
{chr(10).join(state["conversation_history"])}

ç”¨æˆ·å›ç­”è®°å½•ï¼š
{json.dumps(state["user_responses"], ensure_ascii=False, indent=2)}

ç»“æ„åŒ–è¯„ä¼°æ€»ç»“ï¼š
{assessment_summary}

è¯·ç”Ÿæˆä¸€ä»½åŒ…å«ä»¥ä¸‹å†…å®¹çš„ä¸ªæ€§åŒ–æ€»ç»“ï¼š

## ğŸ” è¯„ä¼°æ€»ç»“

## è¯„ä¼°ç»“æœ
- ä¸»è¦çš„éšœç¢ç±»å‹
- å¯ä»¥æ˜¯å¤šä¸ªéšœç¢ç±»å‹
- å¯ä»¥æ˜¯åªæ˜¯æœ‰ç—‡çŠ¶ï¼Œæ²¡æœ‰æ˜ç¡®è¯Šæ–­

## ç°ç—…å²
- åŸºäºå¯¹è¯å†…å®¹æ€»ç»“ç”¨æˆ·çš„ä¸»è¦å›°æ‰°å’Œç—‡çŠ¶è¡¨ç°
- è¯†åˆ«çš„æƒ…æ„Ÿæ¨¡å¼å’Œè¡Œä¸ºç‰¹å¾

## æ—¢å¾€å²
- å¦‚æœæœ‰æ—¢å¾€å²ï¼Œè¯·æ€»ç»“æ—¢å¾€å²

## å®¶æ—å²
- å¦‚æœæœ‰å®¶æ—å²ï¼Œè¯·æ€»ç»“å®¶æ—å²

### ğŸ’¡ å»ºè®®å’Œåç»­æ­¥éª¤
- å…·ä½“çš„ä¸‹ä¸€æ­¥è¡ŒåŠ¨å»ºè®®
- å¯èƒ½æœ‰å¸®åŠ©çš„èµ„æºå’Œæ”¯æŒ

### âš ï¸ é‡è¦è¯´æ˜
- å¼ºè°ƒè¿™æ˜¯ç­›æŸ¥å·¥å…·ï¼Œä¸èƒ½æ›¿ä»£ä¸“ä¸šè¯Šæ–­
- é¼“åŠ±å¯»æ±‚ä¸“ä¸šå¸®åŠ©

è¯·ä¿æŒæ¸©æš–ã€æ”¯æŒæ€§çš„è¯­è°ƒï¼Œé¿å…ä½¿ç”¨å¯èƒ½å¼•èµ·ç„¦è™‘çš„åŒ»å­¦æœ¯è¯­ã€‚"""
        
        print("=" * 50)
        print("ğŸ” DEBUG - GENERATE_SUMMARY LLM CALL")
        print("PROMPT:")
        print(prompt)
        print("=" * 50)
        
        response = self.llm.invoke([HumanMessage(content=prompt)])
        
        print("RESPONSE:")
        print(response.content)
        print("=" * 50)
        
        detailed_summary = response.content
        
        # æ ¹æ®å·¥ä½œæ¨¡å¼æ·»åŠ ä¸åŒçš„ç»“æŸè¯­
        if self.workflow_mode == "structured":
            detailed_summary += """
            
---

ğŸŒŸ **é—®è¯Šè¯„ä¼°å·²å®Œæˆï¼**

ç°åœ¨æˆ‘ä»¬å¯ä»¥è¿›å…¥æ›´è½»æ¾çš„äº¤æµç¯èŠ‚ã€‚å¦‚æœæ‚¨æœ‰ä»»ä½•æƒ³èŠçš„è¯é¢˜ï¼Œæˆ–è€…éœ€è¦æƒ…ç»ªæ”¯æŒå’Œå¿ƒç†å»ºè®®ï¼Œæˆ‘å¾ˆä¹æ„ä»¥CBTç–—æ„ˆå¸ˆçš„èº«ä»½ç»§ç»­é™ªä¼´æ‚¨ã€‚

æ‚¨å¯ä»¥ï¼š
- åˆ†äº«æ‚¨ç°åœ¨çš„æ„Ÿå—
- èŠèŠæ—¥å¸¸ç”Ÿæ´»ä¸­çš„äº‹æƒ…  
- å¯»æ±‚åº”å¯¹å›°éš¾çš„å»ºè®®
- æˆ–è€…ä»»ä½•æ‚¨æƒ³è°ˆè®ºçš„è¯é¢˜

æˆ‘åœ¨è¿™é‡Œé™ªä¼´æ‚¨ ğŸ’•
"""
        
        summary_message = AIMessage(content=detailed_summary)
        
        return {
            **state,
            "messages": state["messages"] + [summary_message],
            "summary": detailed_summary,
            "assessment_complete": True,
            "conversation_mode": "assessment_complete",
            "chat_therapist_active": True
        }
    
    def emergency_response(self, state: InterviewState) -> InterviewState:
        """å¤„ç†ç´§æ€¥æƒ…å†µ"""
        emergency_message = AIMessage(content="""
        ğŸš¨ **æˆ‘å¾ˆæ‹…å¿ƒæ‚¨çš„å®‰å…¨**

        ä»æ‚¨åˆšæ‰çš„è¯ä¸­ï¼Œæˆ‘æ„Ÿå—åˆ°æ‚¨ç°åœ¨å¯èƒ½å¾ˆç—›è‹¦ã€‚æˆ‘æƒ³è®©æ‚¨çŸ¥é“ï¼Œæ‚¨ä¸æ˜¯å­¤å•çš„ï¼Œè€Œä¸”å¯»æ±‚å¸®åŠ©æ˜¯éå¸¸å‹‡æ•¢çš„è¡Œä¸ºã€‚

        **è¯·ç«‹å³é‡‡å–ä»¥ä¸‹è¡ŒåŠ¨ï¼š**

        ğŸ¥ **ç«‹å³å¯»æ±‚ä¸“ä¸šå¸®åŠ©ï¼š**
        - å‰å¾€æœ€è¿‘çš„åŒ»é™¢æ€¥è¯Šç§‘
        - æ‹¨æ‰“ä»¥ä¸‹ç´§æ€¥æ±‚åŠ©çƒ­çº¿ï¼š
          - å…¨å›½å¿ƒç†æ´åŠ©çƒ­çº¿ï¼š**400-161-9995**
          - åŒ—äº¬å±æœºå¹²é¢„çƒ­çº¿ï¼š**400-161-9995** 
          - ä¸Šæµ·å¿ƒç†æ´åŠ©çƒ­çº¿ï¼š**021-34289888**
          - é’å°‘å¹´å¿ƒç†çƒ­çº¿ï¼š**12355**

        ğŸ‘¥ **å¯»æ±‚èº«è¾¹çš„æ”¯æŒï¼š**
        - ç«‹å³è”ç³»ä¿¡ä»»çš„æœ‹å‹æˆ–å®¶äºº
        - è¯·ä»–ä»¬é™ªä¼´åœ¨æ‚¨èº«è¾¹
        - ä¸è¦ç‹¬è‡ªä¸€äºº

        ğŸ›¡ï¸ **ç¡®ä¿ç¯å¢ƒå®‰å…¨ï¼š**
        - ç§»å¼€å¯èƒ½ç”¨äºè‡ªä¼¤çš„ç‰©å“
        - å¾…åœ¨å®‰å…¨ã€æœ‰äººçš„åœ°æ–¹

        **è¯·è®°ä½ï¼š**
        âœ¨ æ‚¨çš„ç”Ÿå‘½å¾ˆå®è´µ  
        âœ¨ ç°åœ¨çš„ç—›è‹¦æ˜¯æš‚æ—¶çš„  
        âœ¨ ä¸“ä¸šå¸®åŠ©æ˜¯æœ‰æ•ˆçš„  
        âœ¨ æœ‰å¾ˆå¤šäººå…³å¿ƒæ‚¨  

        æˆ‘çŸ¥é“ç°åœ¨å¾ˆå›°éš¾ï¼Œä½†è¯·ç›¸ä¿¡æƒ…å†µä¼šå¥½è½¬çš„ã€‚ä¸“ä¸šçš„å¸®åŠ©èƒ½å¤Ÿæ”¯æŒæ‚¨åº¦è¿‡è¿™ä¸ªè‰°éš¾æ—¶æœŸã€‚

        **æ‚¨ç°åœ¨æœ€é‡è¦çš„äº‹æƒ…å°±æ˜¯ç¡®ä¿è‡ªå·±çš„å®‰å…¨ã€‚è¯·ç«‹å³å¯»æ±‚å¸®åŠ©ã€‚**
        """)
        
        return {
            **state,
            "messages": state["messages"] + [emergency_message],
            "assessment_complete": True
        }
    
    async def process_message(self, user_message: str, state: Optional[InterviewState] = None) -> tuple[str, InterviewState]:
        """å¤„ç†ç”¨æˆ·æ¶ˆæ¯"""
        if state is None:
            # åˆæ¬¡å¯¹è¯ï¼Œå¼€å§‹é—®è¯Šæµç¨‹
            result = await self.app.ainvoke({
                "messages": [],
                "current_question_id": None,
                "user_responses": {},
                "assessment_complete": False,
                "emergency_situation": False,
                "summary": "",
                "needs_followup": False,
                "conversation_history": [],
                "followup_questions": [],
                "risk_level": "low",
                "risk_indicators": [],
                "emotional_state": "",
                "content_complete": False,
                "understanding_summary": "",
                "empathetic_response": "",
                "current_analysis": {},
                "debug_info": [],
                "session_start_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "question_sequence": [],
                "final_response": "",
                "assessment_duration": 0,
                "user_engagement_level": "medium",
                "response_detail_level": "moderate",
                "symptoms_identified": [],
                "domains_assessed": [],
                "severity_indicators": {},
                "chief_complaint": "",
                "conversation_mode": "interview",
                "chat_therapist_active": False,
                "mode_detection_result": {},
                "current_topic": "",
                "is_follow_up": False
            })
        else:
            # ç»§ç»­å¯¹è¯ï¼Œæ·»åŠ ç”¨æˆ·æ¶ˆæ¯å¹¶å¤„ç†
            user_msg = HumanMessage(content=user_message)
            updated_state = {
                **state,
                "messages": state["messages"] + [user_msg]
            }
            
            # å…ˆç†è§£å’Œå›åº”ç”¨æˆ·çš„è¯
            analyzed_state = self.understand_and_respond(updated_state)
            
            # ç„¶åç»§ç»­å·¥ä½œæµ
            result = await self.app.ainvoke(analyzed_state)
        
        # è·å–æœ€åä¸€æ¡AIæ¶ˆæ¯
        ai_response = ""
        if result["messages"]:
            for msg in reversed(result["messages"]):
                if isinstance(msg, (AIMessage, SystemMessage)):
                    ai_response = msg.content if hasattr(msg, 'content') else str(msg)
                    break
        
        return ai_response, result

    def process_message_sync(self, user_message: str, state: Optional[dict] = None) -> tuple[str, dict]:
        """å¤„ç†ç”¨æˆ·æ¶ˆæ¯ - åŒæ­¥ç‰ˆæœ¬ï¼Œä½¿ç”¨æ–°çš„å·¥ä½œæµç¨‹"""
        try:
            # å¦‚æœæ˜¯å¼€å§‹è¯„ä¼°æˆ–è€…æ²¡æœ‰çŠ¶æ€ï¼Œä½¿ç”¨å·¥ä½œæµåˆå§‹åŒ–
            if user_message == "å¼€å§‹è¯„ä¼°" or state is None:
                # åˆ›å»ºåˆå§‹çŠ¶æ€ï¼Œä½¿ç”¨start_interviewæ–¹æ³•
                initial_state = self.start_interview({
                    "messages": [],
                    "current_question_id": None,
                    "user_responses": {},
                    "assessment_complete": False,
                    "emergency_situation": False,
                    "summary": "",
                    "needs_followup": False,
                    "conversation_history": [],
                    "chief_complaint": "",
                    "conversation_mode": "idle",
                    "chat_therapist_active": False,
                    "mode_detection_result": {},
                    "conversation_turn_count": 0,
                    "interview_mode_locked": False,
                    "followup_questions": [],
                    "risk_level": "low",
                    "risk_indicators": [],
                    "emotional_state": "",
                    "content_complete": False,
                    "understanding_summary": "",
                    "empathetic_response": "",
                    "current_analysis": {},
                    "debug_info": [],
                    "session_start_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    "question_sequence": [],
                    "final_response": "",
                    "assessment_duration": 0,
                    "user_engagement_level": "medium",
                    "response_detail_level": "moderate",
                    "symptoms_identified": [],
                    "domains_assessed": [],
                    "severity_indicators": {},
                    "current_topic": "",
                    "is_follow_up": False
                })
                
                # ä»messagesä¸­è·å–AIçš„è‡ªæˆ‘ä»‹ç»å›å¤
                ai_response = ""
                if initial_state["messages"]:
                    for msg in reversed(initial_state["messages"]):
                        if isinstance(msg, AIMessage):
                            ai_response = msg.content if hasattr(msg, 'content') else str(msg)
                            break
                
                return ai_response, initial_state
            
            # å¤„ç†ç”¨æˆ·è¾“å…¥ï¼Œä½¿ç”¨æ–°çš„å·¥ä½œæµç¨‹
            if state:
                # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°çŠ¶æ€
                current_state = state.copy()
                current_state["messages"] = current_state.get("messages", []) + [HumanMessage(content=user_message)]
                
                # æ­¥éª¤1: æ£€æµ‹å¯¹è¯æ¨¡å¼
                mode_state = self.detect_conversation_mode(current_state)
                
                # æ ¹æ®æ£€æµ‹åˆ°çš„æ¨¡å¼é€‰æ‹©å¤„ç†è·¯å¾„
                mode = mode_state.get("conversation_mode", "interview")
                
                if mode == "chat" or mode == "assessment_complete":
                    # ä½¿ç”¨CBTç–—æ„ˆå¸ˆå›åº”
                    result_state = self.chat_therapist_response(mode_state)
                    ai_response = result_state.get("final_response", "æˆ‘å¾ˆé«˜å…´å’Œæ‚¨èŠå¤©ï¼Œæœ‰ä»€ä¹ˆæƒ³èŠçš„å—ï¼Ÿ")
                    return ai_response, result_state
                
                elif mode == "interview" or mode == "continue_interview":
                    # ä½¿ç”¨é—®è¯Šæµç¨‹
                    # æ­¥éª¤2: ç†è§£å’Œå›åº”ç”¨æˆ·è¾“å…¥
                    understood_state = self.understand_and_respond(mode_state)
                    
                    # æ­¥éª¤3: æ£€æŸ¥ç´§æ€¥æƒ…å†µ
                    checked_state = self.check_emergency(understood_state)
                    
                    if checked_state.get("emergency_situation", False):
                        # å¤„ç†ç´§æ€¥æƒ…å†µ
                        emergency_state = self.emergency_response(checked_state)
                        # ä»messagesä¸­è·å–ç´§æ€¥å“åº”
                        ai_response = ""
                        if emergency_state["messages"]:
                            for msg in reversed(emergency_state["messages"]):
                                if isinstance(msg, AIMessage):
                                    ai_response = msg.content
                                    break
                        return ai_response, emergency_state
                    
                    elif checked_state.get("assessment_complete", False):
                        # ç”Ÿæˆæ€»ç»“
                        summary_state = self.generate_summary(checked_state)
                        # ä»messagesä¸­è·å–æ€»ç»“
                        ai_response = ""
                        if summary_state["messages"]:
                            for msg in reversed(summary_state["messages"]):
                                if isinstance(msg, AIMessage):
                                    ai_response = msg.content
                                    break
                        return ai_response, summary_state
                    
                    else:
                        # ç»§ç»­æé—®
                        question_state = self.ask_question(checked_state)
                        # ä»messagesä¸­è·å–é—®é¢˜
                        ai_response = ""
                        if question_state["messages"]:
                            for msg in reversed(question_state["messages"]):
                                if isinstance(msg, AIMessage):
                                    ai_response = msg.content
                                    break
                        return ai_response, question_state
                
                else:
                    # é»˜è®¤ä½¿ç”¨é—®è¯Šæ¨¡å¼
                    return self._fallback_response(current_state, user_message)
            
            return "è¯·é‡æ–°å¼€å§‹è¯„ä¼°ã€‚", {}
            
        except Exception as e:
            error_msg = f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„æ¶ˆæ¯æ—¶å‡ºç°é”™è¯¯ï¼š{str(e)}"
            print(f"DEBUG: process_message_sync error: {e}")
            import traceback
            traceback.print_exc()
            return error_msg, state or {}
    
    def _fallback_response(self, state: dict, user_message: str) -> tuple[str, dict]:
        """åå¤‡å“åº”æœºåˆ¶"""
        try:
            fallback_prompt = f"ç”¨æˆ·è¯´ï¼š{user_message}"
            system_content = "ä½ æ˜¯çµæºªæ™ºä¼´ï¼Œä¸€ä½ä¸“ä¸šçš„å¿ƒç†å’¨è¯¢å¸ˆã€‚"
            
            print("=" * 50)
            print("ğŸ” DEBUG - FALLBACK_RESPONSE LLM CALL")
            print("SYSTEM PROMPT:")
            print(system_content)
            print("USER PROMPT:")
            print(fallback_prompt)
            print("=" * 50)
            
            response = self.llm.invoke([
                SystemMessage(content=system_content),
                HumanMessage(content=fallback_prompt)
            ])
            
            print("RESPONSE:")
            print(response.content)
            print("=" * 50)
            
            state["final_response"] = response.content
            return response.content, state
            
        except Exception as e:
            fallback_msg = "æŠ±æ­‰ï¼Œæˆ‘ç°åœ¨æ— æ³•å¾ˆå¥½åœ°å›åº”æ‚¨ã€‚è¯·æ‚¨å†è¯•ä¸€æ¬¡ï¼Œæˆ–è€…å‘Šè¯‰æˆ‘æ‚¨æƒ³è¦è¿›è¡Œé—®è¯Šè¿˜æ˜¯æƒ³è¦é—²èŠã€‚"
            return fallback_msg, state
    
    def _get_next_question_response(self, current_question_id: str, user_response: str, state: dict) -> str:
        """æ ¹æ®å½“å‰é—®é¢˜å’Œç”¨æˆ·å›ç­”ç”Ÿæˆä¸‹ä¸€ä¸ªé—®é¢˜"""
        try:
            # ä½¿ç”¨LLMç”Ÿæˆä¸ªæ€§åŒ–çš„å›åº”å’Œä¸‹ä¸€ä¸ªé—®é¢˜
            conversation_context = "\n".join(state.get("conversation_history", []))[-500:]  # é™åˆ¶é•¿åº¦
            
            # æ ¹æ®debugæ¨¡å¼è°ƒæ•´prompt
            if config.DEBUG:
                prompt = f"""
ä½ çš„åå­—å«é©¬å¿ƒå®ï¼Œæ˜¯ä¸€ä½ä¸“ä¸šçš„å¿ƒç†å’¨è¯¢å¸ˆï¼Œæ­£åœ¨è¿›è¡ŒSCID-5ç»“æ„åŒ–ä¸´åºŠè®¿è°ˆæ¥è¿›è¡Œé¢„é—®è¯Šå’Œå¿ƒç†ç–å¯¼ã€‚

å½“å‰é—®é¢˜ID: {current_question_id}
ç”¨æˆ·å›ç­”: {user_response}
å¯¹è¯å†å²: {conversation_context}

è¯·ä»¥é©¬å¿ƒå®çš„èº«ä»½æ ¹æ®ç”¨æˆ·çš„å›ç­”è¿›è¡Œåˆ†æå’Œå›åº”ï¼š

1. **æƒ…æ„Ÿç†è§£**ï¼šè¯†åˆ«ç”¨æˆ·çš„æƒ…æ„ŸçŠ¶æ€
2. **ç—‡çŠ¶è¯„ä¼°**ï¼šåˆ†ææ˜¯å¦æœ‰å¿ƒç†å¥åº·ç—‡çŠ¶æŒ‡æ ‡
3. **ä¸‹ä¸€æ­¥å†³ç­–**ï¼šé€‰æ‹©æœ€åˆé€‚çš„åç»­é—®é¢˜

å¯é€‰çš„é—®é¢˜é¢†åŸŸï¼š
- æŠ‘éƒç—‡çŠ¶ï¼ˆæƒ…ç»ªä½è½ã€å…´è¶£ç¼ºå¤±ã€ç¡çœ ã€é£Ÿæ¬²ã€ç–²åŠ³ç­‰ï¼‰
- ç„¦è™‘ç—‡çŠ¶ï¼ˆè¿‡åº¦æ‹…å¿ƒã€ææ…Œã€å›é¿è¡Œä¸ºç­‰ï¼‰
- ç²¾ç¥ç—…æ€§ç—‡çŠ¶ï¼ˆå¹»è§‰ã€å¦„æƒ³ç­‰ï¼‰
- ç‰©è´¨ä½¿ç”¨é—®é¢˜
- åˆ›ä¼¤ç»å†

è¯·ä»¥JSONæ ¼å¼å›å¤ï¼Œç”¨äºdebugåˆ†æï¼š
{{
    "emotional_analysis": "ç”¨æˆ·æƒ…æ„ŸçŠ¶æ€åˆ†æ",
    "symptom_indicators": ["å‘ç°çš„ç—‡çŠ¶æŒ‡æ ‡"],
    "risk_assessment": "é£é™©è¯„ä¼°",
    "next_question_rationale": "é€‰æ‹©ä¸‹ä¸€ä¸ªé—®é¢˜çš„ç†ç”±",
    "user_response": "é©¬å¿ƒå®çš„è‡ªç„¶å…±æƒ…å›åº”å’Œä¸‹ä¸€ä¸ªé—®é¢˜",
    "assessment_complete": false
}}

å¦‚æœè¯„ä¼°åº”è¯¥ç»“æŸï¼Œè¯·è®¾ç½®"assessment_complete": trueã€‚
"""
            else:
                # è·å–ç”¨æˆ·ä¸»è¯‰
                chief_complaint = state.get("chief_complaint", "")
                
                prompt = f"""
ä½ çš„åå­—å«é©¬å¿ƒå®ï¼Œæ˜¯ä¸€ä½æ¸©æš–ã€ä¸“ä¸šçš„å¿ƒç†å’¨è¯¢å¸ˆï¼Œæ­£åœ¨ä¸æ¥è®¿è€…è¿›è¡Œæ¸©å’Œçš„å¯¹è¯äº†è§£ï¼Œè¿›è¡Œé¢„é—®è¯Šå’Œå¿ƒç†ç–å¯¼ã€‚

ç”¨æˆ·çš„ä¸»è¯‰ï¼š"{chief_complaint}"
ç”¨æˆ·åˆšæ‰è¯´ï¼š"{user_response}"

è¯·ä»¥é©¬å¿ƒå®çš„èº«ä»½åƒçœŸæ­£çš„å¿ƒç†å’¨è¯¢å¸ˆä¸€æ ·å›åº”ï¼š
1. é¦–å…ˆè¡¨è¾¾ç†è§£å’Œå…±æƒ…
2. åŸºäºç”¨æˆ·çš„ä¸»è¯‰å’Œå½“å‰å›ç­”ï¼Œè‡ªç„¶åœ°å¼•å‡ºä¸‹ä¸€ä¸ªç›¸å…³é—®é¢˜
3. ä¿æŒå¯¹è¯çš„è‡ªç„¶æµç•…ï¼Œä¸è¦æ˜¾å¾—è¿‡äºç»“æ„åŒ–
4. å›´ç»•ç”¨æˆ·çš„ä¸»è¦å›°æ‰°è¿›è¡Œæ·±å…¥äº†è§£

å¯¹è¯é£æ ¼è¦æ±‚ï¼š
- æ¸©æš–ã€ç†è§£ã€è‡ªç„¶
- åƒæœ‹å‹èˆ¬çš„ä¸“ä¸šå…³æ€€
- é¿å…æ˜æ˜¾çš„"é—®å·å¼"æé—®
- æ ¹æ®ç”¨æˆ·çš„ä¸»è¯‰å’Œå›ç­”å†…å®¹è‡ªç„¶å»¶å±•
- ä½“ç°é©¬å¿ƒå®ä¸“ä¸šè€Œæ¸©æš–çš„ä¸ªäººé£æ ¼

é‡ç‚¹å…³æ³¨é¢†åŸŸï¼ˆä½†è¦ç»“åˆç”¨æˆ·ä¸»è¯‰è¿›è¡Œä¸ªæ€§åŒ–è¯¢é—®ï¼‰ï¼š
- ä¸ä¸»è¯‰ç›¸å…³çš„å…·ä½“ç—‡çŠ¶å’Œè¡¨ç°
- é—®é¢˜çš„æŒç»­æ—¶é—´å’Œå‘å±•è¿‡ç¨‹
- å¯¹æ—¥å¸¸ç”Ÿæ´»çš„å…·ä½“å½±å“
- æƒ…ç»ªçŠ¶æ€å’Œå¿ƒå¢ƒå˜åŒ–
- ç¡çœ ã€é£Ÿæ¬²ã€ç²¾åŠ›ç­‰ç”Ÿç†æ–¹é¢
- ç¤¾äº¤å’Œå·¥ä½œæƒ…å†µ
- æ—¢å¾€çš„åº”å¯¹æ–¹å¼å’Œæ±‚åŠ©ç»å†

è¯·ç›´æ¥ç»™å‡ºè‡ªç„¶çš„å¯¹è¯å›åº”ï¼Œä¸éœ€è¦ä»»ä½•æ ¼å¼æ ‡è®°ã€‚å¦‚æœè§‰å¾—å·²ç»äº†è§£è¶³å¤Ÿä¿¡æ¯ï¼Œå¯ä»¥å¼€å§‹æ€»ç»“ã€‚

å¦‚æœéœ€è¦ç»“æŸè¯„ä¼°ï¼Œè¯·åœ¨å›åº”æœ€ååŠ ä¸Š"[ASSESSMENT_COMPLETE]"ã€‚
"""
            
            print("=" * 50)
            print("ğŸ” DEBUG - GET_NEXT_QUESTION_RESPONSE LLM CALL")
            print("PROMPT:")
            print(prompt)
            print("=" * 50)
            
            response = self.llm.invoke([HumanMessage(content=prompt)])
            
            print("RESPONSE:")
            print(response.content)
            print("=" * 50)
            
            ai_response = response.content
            
            # å¤„ç†debugæ¨¡å¼çš„JSONå›å¤
            if config.DEBUG:
                try:
                    import json
                    debug_data = json.loads(ai_response)
                    
                    # å­˜å‚¨debugä¿¡æ¯åˆ°çŠ¶æ€ä¸­
                    if "debug_info" not in state:
                        state["debug_info"] = []
                    
                    state["debug_info"].append({
                        "question_id": current_question_id,
                        "user_input": user_response,
                        "analysis": debug_data
                    })
                    
                    # æ£€æŸ¥æ˜¯å¦å®Œæˆè¯„ä¼°
                    if debug_data.get("assessment_complete", False):
                        state["assessment_complete"] = True
                        return self._generate_assessment_summary(state)
                    
                    return debug_data.get("user_response", "è¯·ç»§ç»­åˆ†äº«æ‚¨çš„æ„Ÿå—ã€‚")
                    
                except json.JSONDecodeError:
                    # å¦‚æœJSONè§£æå¤±è´¥ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹å›å¤
                    pass
            
            # å¤„ç†æ­£å¸¸æ¨¡å¼çš„å›å¤
            if "[ASSESSMENT_COMPLETE]" in ai_response:
                state["assessment_complete"] = True
                ai_response = ai_response.replace("[ASSESSMENT_COMPLETE]", "").strip()
                # å…ˆç»™å‡ºå½“å‰å›åº”ï¼Œç„¶åç”Ÿæˆæ€»ç»“
                state["final_response"] = ai_response
                summary_response = self._generate_assessment_summary(state)
                return f"{ai_response}\n\n{summary_response}"
            
            return ai_response
            
        except Exception as e:
            # å¦‚æœLLMè°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨ç®€å•çš„è§„åˆ™
            return self._fallback_question_logic(current_question_id, user_response, state)
    
    def _fallback_question_logic(self, current_question_id: str, user_response: str, state: dict) -> str:
        """å¤‡ç”¨çš„ç®€å•è§„åˆ™é€»è¾‘"""
        responses_count = len(state.get("user_responses", {}))
        
        if responses_count >= 3:
            state["assessment_complete"] = True
            return self._generate_simple_summary(state)
        
        # ç®€å•çš„é—®é¢˜åºåˆ—
        if current_question_id == "depression_mood":
            return "è°¢è°¢æ‚¨çš„åˆ†äº«ã€‚æˆ‘æƒ³äº†è§£ä¸€ä¸‹æ‚¨å¹³æ—¶çš„ç”Ÿæ´»çŠ¶æ€ï¼Œæ‚¨æœ€è¿‘è¿˜åƒä»¥å‰ä¸€æ ·å¯¹è‡ªå·±å–œæ¬¢çš„æ´»åŠ¨æ„Ÿåˆ°æœ‰è¶£å’Œå¼€å¿ƒå—ï¼Ÿæ¯”å¦‚çœ‹ç”µå½±ã€å’Œæœ‹å‹èŠå¤©ã€å¬éŸ³ä¹ï¼Œæˆ–è€…å…¶ä»–æ‚¨ä»¥å‰å–œæ¬¢åšçš„äº‹æƒ…ï¼Ÿ"
        elif "depression" in current_question_id:
            return "äº†è§£äº†ã€‚æˆ‘è¿˜æƒ³äº†è§£ä¸€ä¸‹æ‚¨æœ€è¿‘æ˜¯å¦æœ‰ä¸€äº›æ‹…å¿ƒæˆ–è€…ç„¦è™‘çš„æ„Ÿå—ï¼Ÿæ¯”å¦‚å¯¹æœªæ¥çš„æ‹…å¿§ï¼Œæˆ–è€…æ€»æ˜¯æ”¾ä¸ä¸‹å¿ƒæ¥çš„äº‹æƒ…ï¼Ÿ"
        else:
            state["assessment_complete"] = True
            return self._generate_simple_summary(state)
    
    def _generate_assessment_summary(self, state: dict) -> str:
        """ä½¿ç”¨LLMç”Ÿæˆè¯„ä¼°æ€»ç»“"""
        try:
            responses = state.get("user_responses", {})
            conversation = "\n".join(state.get("conversation_history", []))
            chief_complaint = state.get("chief_complaint", "")
            
            prompt = f"""
åŸºäºä»¥ä¸‹å¿ƒç†å¥åº·ç­›æŸ¥å¯¹è¯ï¼Œç”Ÿæˆä¸€ä»½ä¸“ä¸šã€æ¸©æš–çš„è¯„ä¼°æ€»ç»“ï¼š

ç”¨æˆ·ä¸»è¯‰ï¼š
{chief_complaint}

ç”¨æˆ·å›ç­”è®°å½•ï¼š
{json.dumps(responses, ensure_ascii=False, indent=2)}

å¯¹è¯å†å²ï¼š
{conversation}

        è¯·ç”ŸæˆåŒ…å«ä»¥ä¸‹å†…å®¹çš„æ€»ç»“ï¼š

        ## ğŸ” è¯„ä¼°æ€»ç»“

        ### ç”¨æˆ·ä¸»è¯‰
        [ç®€è¦é‡è¿°ç”¨æˆ·çš„ä¸»è¦å›°æ‰°å’Œæ±‚åŠ©åŸå› ]

        ### ä¸»è¦å‘ç°
        [åŸºäºå¯¹è¯æ€»ç»“ç”¨æˆ·çš„ä¸»è¦å›°æ‰°å’Œç—‡çŠ¶è¡¨ç°ï¼Œé‡ç‚¹åˆ†æä¸ä¸»è¯‰ç›¸å…³çš„å†…å®¹]

        ### é£é™©è¯„ä¼°
        [è¯„ä¼°å½“å‰çš„å¿ƒç†å¥åº·é£é™©ç­‰çº§]

        ### ğŸ’¡ é’ˆå¯¹æ€§å»ºè®®
        [åŸºäºç”¨æˆ·ä¸»è¯‰å’Œç—‡çŠ¶ï¼Œæä¾›å…·ä½“çš„å»ºè®®å’Œåç»­æ­¥éª¤]

        ### âš ï¸ é‡è¦è¯´æ˜
        - æ­¤è¯„ä¼°ä»…ä¾›å‚è€ƒï¼Œä¸èƒ½æ›¿ä»£ä¸“ä¸šåŒ»ç–—è¯Šæ–­
        - å¦‚æœ‰æŒç»­ç—‡çŠ¶ï¼Œå»ºè®®å’¨è¯¢ä¸“ä¸šå¿ƒç†å¥åº·æœåŠ¡
        - å¦‚æœ‰ç´§æ€¥æƒ…å†µï¼Œè¯·ç«‹å³æ‹¨æ‰“ï¼š400-161-9995

        è¯·ä¿æŒæ¸©æš–ã€æ”¯æŒæ€§çš„è¯­è°ƒï¼Œä½“ç°å¯¹ç”¨æˆ·ä¸»è¯‰çš„ç†è§£å’Œå…³æ³¨ã€‚
"""
            
            print("=" * 50)
            print("ğŸ” DEBUG - GENERATE_ASSESSMENT_SUMMARY LLM CALL")
            print("PROMPT:")
            print(prompt)
            print("=" * 50)
            
            response = self.llm.invoke([HumanMessage(content=prompt)])
            
            print("RESPONSE:")
            print(response.content)
            print("=" * 50)
            
            return response.content
            
        except Exception:
            return self._generate_simple_summary(state)
    
    def _generate_simple_summary(self, state: dict) -> str:
        """ç”Ÿæˆç®€å•çš„è¯„ä¼°æ€»ç»“"""
        return """## ğŸ” è¯„ä¼°æ€»ç»“

æ„Ÿè°¢æ‚¨å®Œæˆè¿™æ¬¡å¿ƒç†å¥åº·ç­›æŸ¥ã€‚

### ğŸ’¡ å»ºè®®
- å¦‚æœ‰ä»»ä½•å›°æ‰°æˆ–ç—‡çŠ¶æŒç»­ï¼Œå»ºè®®å’¨è¯¢ä¸“ä¸šå¿ƒç†å¥åº·æœåŠ¡
- ä¿æŒè‰¯å¥½çš„ä½œæ¯å’Œç”Ÿæ´»ä¹ æƒ¯
- å¯»æ±‚å®¶äººæœ‹å‹çš„æ”¯æŒ

### âš ï¸ é‡è¦è¯´æ˜
- æ­¤è¯„ä¼°ä»…ä¾›å‚è€ƒï¼Œä¸èƒ½æ›¿ä»£ä¸“ä¸šåŒ»ç–—è¯Šæ–­
- å¦‚æœ‰ç´§æ€¥æƒ…å†µï¼Œè¯·ç«‹å³æ‹¨æ‰“ï¼š400-161-9995

**æ‚¨çš„å¿ƒç†å¥åº·å¾ˆé‡è¦ï¼Œè¯·ä¸è¦çŠ¹è±«å¯»æ±‚ä¸“ä¸šå¸®åŠ©ã€‚**"""

# å…¨å±€ä»£ç†å®ä¾‹
# å¯ä»¥é€šè¿‡ä¿®æ”¹è¿™é‡Œçš„workflow_modeå‚æ•°æ¥åˆ‡æ¢å·¥ä½œæ¨¡å¼ï¼š
# - "adaptive": æ™ºèƒ½æ£€æµ‹æ¨¡å¼ï¼Œæ ¹æ®ç”¨æˆ·æ„å›¾è‡ªç”±åˆ‡æ¢é—®è¯Šå’Œé—²èŠ
# - "structured": å›ºå®šæµç¨‹æ¨¡å¼ï¼Œå…ˆå®Œæˆé—®è¯Šå†è½¬CBTé—²èŠ
scid5_agent = SCID5Agent(workflow_mode="adaptive")  # é»˜è®¤ä½¿ç”¨æ™ºèƒ½æ£€æµ‹æ¨¡å¼

def create_agent(workflow_mode: str = "adaptive") -> SCID5Agent:
    """
    åˆ›å»ºSCID5ä»£ç†å®ä¾‹
    
    Args:
        workflow_mode: å·¥ä½œæµç¨‹æ¨¡å¼
            - "adaptive": æ™ºèƒ½æ£€æµ‹æ¨¡å¼ï¼Œæ ¹æ®ç”¨æˆ·æ„å›¾è‡ªç”±åˆ‡æ¢é—®è¯Šå’Œé—²èŠ
            - "structured": å›ºå®šæµç¨‹æ¨¡å¼ï¼Œå…ˆå®Œæˆé—®è¯Šå†è½¬CBTé—²èŠ
    
    Returns:
        SCID5Agent: é…ç½®å¥½çš„ä»£ç†å®ä¾‹
    """
    return SCID5Agent(workflow_mode=workflow_mode) 