"""
å›åº”ç”Ÿæˆæ¨¡å—
åŒ…å«è¯„ä¼°æ€»ç»“ç”Ÿæˆå’Œå„ç§æ™ºèƒ½å›åº”åŠŸèƒ½
"""

import json
from typing import Tuple
from langchain.schema import HumanMessage, SystemMessage, AIMessage

from agent import InterviewState
from scid5_knowledge import scid5_kb
from config import config
from prompts import PromptTemplates


class ResponseGenerator:
    """å›åº”ç”Ÿæˆå™¨"""
    
    def __init__(self, llm):
        self.llm = llm
    
    def generate_summary(self, state: InterviewState) -> InterviewState:
        """ç”Ÿæˆè¯„ä¼°æ€»ç»“"""
        assessment_summary = scid5_kb.generate_assessment_summary()
        
        # ä½¿ç”¨LLMç”Ÿæˆæ›´è¯¦ç»†å’Œä¸ªæ€§åŒ–çš„æ€»ç»“
        prompt = PromptTemplates.get_assessment_summary_prompt(
            state["conversation_history"], 
            state["user_responses"], 
            assessment_summary
        )
        
        print("=" * 50, flush=True)
        print("ğŸ” DEBUG - GENERATE_SUMMARY LLM CALL", flush=True)
        print("PROMPT:", flush=True)
        print(prompt, flush=True)
        print("=" * 50, flush=True)
        
        response = self.llm.invoke([HumanMessage(content=prompt)])
        
        print("RESPONSE:", flush=True)
        print(response.content, flush=True)
        print("=" * 50, flush=True)
        
        detailed_summary = response.content
        
        # æ ¹æ®å·¥ä½œæ¨¡å¼æ·»åŠ ä¸åŒçš„ç»“æŸè¯­
        workflow_mode = getattr(self, 'workflow_mode', 'adaptive')
        if workflow_mode == "structured":
            detailed_summary += PromptTemplates.STRUCTURED_MODE_COMPLETION_MESSAGE
        
        summary_message = AIMessage(content=detailed_summary)
        
        return {
            **state,
            "messages": state["messages"] + [summary_message],
            "summary": detailed_summary,
            "assessment_complete": True,
            "conversation_mode": "assessment_complete",
            "chat_therapist_active": True
        }
    
    def get_next_question_response(self, current_question_id: str, user_response: str, state: dict) -> str:
        """æ ¹æ®å½“å‰é—®é¢˜å’Œç”¨æˆ·å›ç­”ç”Ÿæˆä¸‹ä¸€ä¸ªé—®é¢˜"""
        try:
            # ä½¿ç”¨LLMç”Ÿæˆä¸ªæ€§åŒ–çš„å›åº”å’Œä¸‹ä¸€ä¸ªé—®é¢˜
            conversation_context = "\n".join(state.get("conversation_history", []))[-500:]  # é™åˆ¶é•¿åº¦
            
            # æ ¹æ®debugæ¨¡å¼è°ƒæ•´prompt
            if config.DEBUG:
                prompt = f"""
ä½ çš„åå­—å«é©¬å¿ƒå®ï¼Œæ˜¯ä¸€ä½ä¸“ä¸šçš„å¿ƒç†å’¨è¯¢å¸ˆï¼Œæ­£åœ¨è¿›è¡ŒSCID-5ç»“æ„åŒ–ä¸´åºŠè®¿è°ˆæ¥è¿›è¡Œé¢„é—®è¯Šå’Œå¿ƒç†ç–å¯¼ã€‚

å½“å‰é—®é¢˜ID: {current_question_id}
ç”¨æˆ·å›ç­”: {user_response}
å¯¹è¯å†å²: {conversation_context}

è¯·ä»¥é©¬å¿ƒå®çš„èº«ä»½æ ¹æ®ç”¨æˆ·çš„å›ç­”è¿›è¡Œåˆ†æå’Œå›åº”ï¼š

1. **æƒ…æ„Ÿç†è§£**ï¼šè¯†åˆ«ç”¨æˆ·çš„æƒ…æ„ŸçŠ¶æ€
2. **ç—‡çŠ¶è¯„ä¼°**ï¼šåˆ†ææ˜¯å¦æœ‰å¿ƒç†å¥åº·ç—‡çŠ¶æŒ‡æ ‡
3. **ä¸‹ä¸€æ­¥å†³ç­–**ï¼šé€‰æ‹©æœ€åˆé€‚çš„åç»­é—®é¢˜

å¯é€‰çš„é—®é¢˜é¢†åŸŸï¼š
- æŠ‘éƒç—‡çŠ¶ï¼ˆæƒ…ç»ªä½è½ã€å…´è¶£ç¼ºå¤±ã€ç¡çœ ã€é£Ÿæ¬²ã€ç–²åŠ³ç­‰ï¼‰
- ç„¦è™‘ç—‡çŠ¶ï¼ˆè¿‡åº¦æ‹…å¿ƒã€ææ…Œã€å›é¿è¡Œä¸ºç­‰ï¼‰
- ç²¾ç¥ç—…æ€§ç—‡çŠ¶ï¼ˆå¹»è§‰ã€å¦„æƒ³ç­‰ï¼‰
- ç‰©è´¨ä½¿ç”¨é—®é¢˜
- åˆ›ä¼¤ç»å†

è¯·ä»¥JSONæ ¼å¼å›å¤ï¼Œç”¨äºdebugåˆ†æï¼š
{{
    "emotional_analysis": "ç”¨æˆ·æƒ…æ„ŸçŠ¶æ€åˆ†æ",
    "symptom_indicators": ["å‘ç°çš„ç—‡çŠ¶æŒ‡æ ‡"],
    "risk_assessment": "é£é™©è¯„ä¼°",
    "next_question_rationale": "é€‰æ‹©ä¸‹ä¸€ä¸ªé—®é¢˜çš„ç†ç”±",
    "user_response": "é©¬å¿ƒå®çš„è‡ªç„¶å…±æƒ…å›åº”å’Œä¸‹ä¸€ä¸ªé—®é¢˜",
    "assessment_complete": false
}}

å¦‚æœè¯„ä¼°åº”è¯¥ç»“æŸï¼Œè¯·è®¾ç½®"assessment_complete": trueã€‚
"""
            else:
                # è·å–ç”¨æˆ·ä¸»è¯‰
                chief_complaint = state.get("chief_complaint", "")
                
                prompt = f"""
ä½ çš„åå­—å«é©¬å¿ƒå®ï¼Œæ˜¯ä¸€ä½æ¸©æš–ã€ä¸“ä¸šçš„å¿ƒç†å’¨è¯¢å¸ˆï¼Œæ­£åœ¨ä¸æ¥è®¿è€…è¿›è¡Œæ¸©å’Œçš„å¯¹è¯äº†è§£ï¼Œè¿›è¡Œé¢„é—®è¯Šå’Œå¿ƒç†ç–å¯¼ã€‚

ç”¨æˆ·çš„ä¸»è¯‰ï¼š"{chief_complaint}"
ç”¨æˆ·åˆšæ‰è¯´ï¼š"{user_response}"

è¯·ä»¥é©¬å¿ƒå®çš„èº«ä»½åƒçœŸæ­£çš„å¿ƒç†å’¨è¯¢å¸ˆä¸€æ ·å›åº”ï¼š
1. é¦–å…ˆè¡¨è¾¾ç†è§£å’Œå…±æƒ…
2. åŸºäºç”¨æˆ·çš„ä¸»è¯‰å’Œå½“å‰å›ç­”ï¼Œè‡ªç„¶åœ°å¼•å‡ºä¸‹ä¸€ä¸ªç›¸å…³é—®é¢˜
3. ä¿æŒå¯¹è¯çš„è‡ªç„¶æµç•…ï¼Œä¸è¦æ˜¾å¾—è¿‡äºç»“æ„åŒ–
4. å›´ç»•ç”¨æˆ·çš„ä¸»è¦å›°æ‰°è¿›è¡Œæ·±å…¥äº†è§£

å¯¹è¯é£æ ¼è¦æ±‚ï¼š
- æ¸©æš–ã€ç†è§£ã€è‡ªç„¶
- åƒæœ‹å‹èˆ¬çš„ä¸“ä¸šå…³æ€€
- é¿å…æ˜æ˜¾çš„"é—®å·å¼"æé—®
- æ ¹æ®ç”¨æˆ·çš„ä¸»è¯‰å’Œå›ç­”å†…å®¹è‡ªç„¶å»¶å±•
- ä½“ç°é©¬å¿ƒå®ä¸“ä¸šè€Œæ¸©æš–çš„ä¸ªäººé£æ ¼

é‡ç‚¹å…³æ³¨é¢†åŸŸï¼ˆä½†è¦ç»“åˆç”¨æˆ·ä¸»è¯‰è¿›è¡Œä¸ªæ€§åŒ–è¯¢é—®ï¼‰ï¼š
- ä¸ä¸»è¯‰ç›¸å…³çš„å…·ä½“ç—‡çŠ¶å’Œè¡¨ç°
- é—®é¢˜çš„æŒç»­æ—¶é—´å’Œå‘å±•è¿‡ç¨‹
- å¯¹æ—¥å¸¸ç”Ÿæ´»çš„å…·ä½“å½±å“
- æƒ…ç»ªçŠ¶æ€å’Œå¿ƒå¢ƒå˜åŒ–
- ç¡çœ ã€é£Ÿæ¬²ã€ç²¾åŠ›ç­‰ç”Ÿç†æ–¹é¢
- ç¤¾äº¤å’Œå·¥ä½œæƒ…å†µ
- æ—¢å¾€çš„åº”å¯¹æ–¹å¼å’Œæ±‚åŠ©ç»å†

è¯·ç›´æ¥ç»™å‡ºè‡ªç„¶çš„å¯¹è¯å›åº”ï¼Œä¸éœ€è¦ä»»ä½•æ ¼å¼æ ‡è®°ã€‚å¦‚æœè§‰å¾—å·²ç»äº†è§£è¶³å¤Ÿä¿¡æ¯ï¼Œå¯ä»¥å¼€å§‹æ€»ç»“ã€‚

å¦‚æœéœ€è¦ç»“æŸè¯„ä¼°ï¼Œè¯·åœ¨å›åº”æœ€ååŠ ä¸Š"[ASSESSMENT_COMPLETE]"ã€‚
"""
            
            print("=" * 50, flush=True)
            print("ğŸ” DEBUG - GET_NEXT_QUESTION_RESPONSE LLM CALL", flush=True)
            print("PROMPT:", flush=True)
            print(prompt, flush=True)
            print("=" * 50, flush=True)
            
            response = self.llm.invoke([HumanMessage(content=prompt)])
            
            print("RESPONSE:", flush=True)
            print(response.content, flush=True)
            print("=" * 50, flush=True)
            
            ai_response = response.content
            
            # å¤„ç†debugæ¨¡å¼çš„JSONå›å¤
            if config.DEBUG:
                try:
                    debug_data = json.loads(ai_response)
                    
                    # å­˜å‚¨debugä¿¡æ¯åˆ°çŠ¶æ€ä¸­
                    if "debug_info" not in state:
                        state["debug_info"] = []
                    
                    state["debug_info"].append({
                        "question_id": current_question_id,
                        "user_input": user_response,
                        "analysis": debug_data
                    })
                    
                    # æ£€æŸ¥æ˜¯å¦å®Œæˆè¯„ä¼°
                    if debug_data.get("assessment_complete", False):
                        state["assessment_complete"] = True
                        return self.generate_assessment_summary(state)
                    
                    return debug_data.get("user_response", "è¯·ç»§ç»­åˆ†äº«æ‚¨çš„æ„Ÿå—ã€‚")
                    
                except json.JSONDecodeError:
                    # å¦‚æœJSONè§£æå¤±è´¥ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹å›å¤
                    pass
            
            # å¤„ç†æ­£å¸¸æ¨¡å¼çš„å›å¤
            if "[ASSESSMENT_COMPLETE]" in ai_response:
                state["assessment_complete"] = True
                ai_response = ai_response.replace("[ASSESSMENT_COMPLETE]", "").strip()
                # å…ˆç»™å‡ºå½“å‰å›åº”ï¼Œç„¶åç”Ÿæˆæ€»ç»“
                state["final_response"] = ai_response
                summary_response = self.generate_assessment_summary(state)
                return f"{ai_response}\n\n{summary_response}"
            
            return ai_response
            
        except Exception as e:
            # å¦‚æœLLMè°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨ç®€å•çš„è§„åˆ™
            return self.fallback_question_logic(current_question_id, user_response, state)
    
    def fallback_question_logic(self, current_question_id: str, user_response: str, state: dict) -> str:
        """å¤‡ç”¨çš„ç®€å•è§„åˆ™é€»è¾‘"""
        responses_count = len(state.get("user_responses", {}))
        
        if responses_count >= 3:
            state["assessment_complete"] = True
            return self.generate_simple_summary(state)
        
        # ç®€å•çš„é—®é¢˜åºåˆ—
        if current_question_id == "depression_mood":
            return "è°¢è°¢æ‚¨çš„åˆ†äº«ã€‚æˆ‘æƒ³äº†è§£ä¸€ä¸‹æ‚¨å¹³æ—¶çš„ç”Ÿæ´»çŠ¶æ€ï¼Œæ‚¨æœ€è¿‘è¿˜åƒä»¥å‰ä¸€æ ·å¯¹è‡ªå·±å–œæ¬¢çš„æ´»åŠ¨æ„Ÿåˆ°æœ‰è¶£å’Œå¼€å¿ƒå—ï¼Ÿæ¯”å¦‚çœ‹ç”µå½±ã€å’Œæœ‹å‹èŠå¤©ã€å¬éŸ³ä¹ï¼Œæˆ–è€…å…¶ä»–æ‚¨ä»¥å‰å–œæ¬¢åšçš„äº‹æƒ…ï¼Ÿ"
        elif "depression" in current_question_id:
            return "äº†è§£äº†ã€‚æˆ‘è¿˜æƒ³äº†è§£ä¸€ä¸‹æ‚¨æœ€è¿‘æ˜¯å¦æœ‰ä¸€äº›æ‹…å¿ƒæˆ–è€…ç„¦è™‘çš„æ„Ÿå—ï¼Ÿæ¯”å¦‚å¯¹æœªæ¥çš„æ‹…å¿§ï¼Œæˆ–è€…æ€»æ˜¯æ”¾ä¸ä¸‹å¿ƒæ¥çš„äº‹æƒ…ï¼Ÿ"
        else:
            state["assessment_complete"] = True
            return self.generate_simple_summary(state)
    
    def generate_assessment_summary(self, state: dict) -> str:
        """ä½¿ç”¨LLMç”Ÿæˆè¯„ä¼°æ€»ç»“"""
        try:
            responses = state.get("user_responses", {})
            conversation = "\n".join(state.get("conversation_history", []))
            chief_complaint = state.get("chief_complaint", "")
            
            prompt = f"""
åŸºäºä»¥ä¸‹å¿ƒç†å¥åº·ç­›æŸ¥å¯¹è¯ï¼Œç”Ÿæˆä¸€ä»½ä¸“ä¸šã€æ¸©æš–çš„è¯„ä¼°æ€»ç»“ï¼š

ç”¨æˆ·ä¸»è¯‰ï¼š
{chief_complaint}

ç”¨æˆ·å›ç­”è®°å½•ï¼š
{json.dumps(responses, ensure_ascii=False, indent=2)}

å¯¹è¯å†å²ï¼š
{conversation}

        è¯·ç”ŸæˆåŒ…å«ä»¥ä¸‹å†…å®¹çš„æ€»ç»“ï¼š

        ## ğŸ” è¯„ä¼°æ€»ç»“

        ### ç”¨æˆ·ä¸»è¯‰
        [ç®€è¦é‡è¿°ç”¨æˆ·çš„ä¸»è¦å›°æ‰°å’Œæ±‚åŠ©åŸå› ]

        ### ä¸»è¦å‘ç°
        [åŸºäºå¯¹è¯æ€»ç»“ç”¨æˆ·çš„ä¸»è¦å›°æ‰°å’Œç—‡çŠ¶è¡¨ç°ï¼Œé‡ç‚¹åˆ†æä¸ä¸»è¯‰ç›¸å…³çš„å†…å®¹]

        ### é£é™©è¯„ä¼°
        [è¯„ä¼°å½“å‰çš„å¿ƒç†å¥åº·é£é™©ç­‰çº§]

        ### ğŸ’¡ é’ˆå¯¹æ€§å»ºè®®
        [åŸºäºç”¨æˆ·ä¸»è¯‰å’Œç—‡çŠ¶ï¼Œæä¾›å…·ä½“çš„å»ºè®®å’Œåç»­æ­¥éª¤]

        ### âš ï¸ é‡è¦è¯´æ˜
        - æ­¤è¯„ä¼°ä»…ä¾›å‚è€ƒï¼Œä¸èƒ½æ›¿ä»£ä¸“ä¸šåŒ»ç–—è¯Šæ–­
        - å¦‚æœ‰æŒç»­ç—‡çŠ¶ï¼Œå»ºè®®å’¨è¯¢ä¸“ä¸šå¿ƒç†å¥åº·æœåŠ¡
        - å¦‚æœ‰ç´§æ€¥æƒ…å†µï¼Œè¯·ç«‹å³æ‹¨æ‰“ï¼š400-161-9995

        è¯·ä¿æŒæ¸©æš–ã€æ”¯æŒæ€§çš„è¯­è°ƒï¼Œä½“ç°å¯¹ç”¨æˆ·ä¸»è¯‰çš„ç†è§£å’Œå…³æ³¨ã€‚
"""
            
            print("=" * 50, flush=True)
            print("ğŸ” DEBUG - GENERATE_ASSESSMENT_SUMMARY LLM CALL", flush=True)
            print("PROMPT:", flush=True)
            print(prompt, flush=True)
            print("=" * 50, flush=True)
            
            response = self.llm.invoke([HumanMessage(content=prompt)])
            
            print("RESPONSE:", flush=True)
            print(response.content, flush=True)
            print("=" * 50, flush=True)
            
            return response.content
            
        except Exception:
            return self.generate_simple_summary(state)
    
    def generate_simple_summary(self, state: dict) -> str:
        """ç”Ÿæˆç®€å•çš„è¯„ä¼°æ€»ç»“"""
        return """## ğŸ” è¯„ä¼°æ€»ç»“

æ„Ÿè°¢æ‚¨å®Œæˆè¿™æ¬¡å¿ƒç†å¥åº·ç­›æŸ¥ã€‚

### ğŸ’¡ å»ºè®®
- å¦‚æœ‰ä»»ä½•å›°æ‰°æˆ–ç—‡çŠ¶æŒç»­ï¼Œå»ºè®®å’¨è¯¢ä¸“ä¸šå¿ƒç†å¥åº·æœåŠ¡
- ä¿æŒè‰¯å¥½çš„ä½œæ¯å’Œç”Ÿæ´»ä¹ æƒ¯
- å¯»æ±‚å®¶äººæœ‹å‹çš„æ”¯æŒ

### âš ï¸ é‡è¦è¯´æ˜
- æ­¤è¯„ä¼°ä»…ä¾›å‚è€ƒï¼Œä¸èƒ½æ›¿ä»£ä¸“ä¸šåŒ»ç–—è¯Šæ–­
- å¦‚æœ‰ç´§æ€¥æƒ…å†µï¼Œè¯·ç«‹å³æ‹¨æ‰“ï¼š400-161-9995

**æ‚¨çš„å¿ƒç†å¥åº·å¾ˆé‡è¦ï¼Œè¯·ä¸è¦çŠ¹è±«å¯»æ±‚ä¸“ä¸šå¸®åŠ©ã€‚**"""
    
    def fallback_response(self, state: dict, user_message: str) -> Tuple[str, dict]:
        """åå¤‡å“åº”æœºåˆ¶"""
        try:
            # æ›´æ–°å¯¹è¯å†å² - æ·»åŠ ä¸Šä¸€è½®AIå›å¤å’Œå½“å‰ç”¨æˆ·æ¶ˆæ¯
            updated_history = state.get("conversation_history", []).copy()
            
            # æ·»åŠ ä¸Šä¸€è½®çš„AIå›å¤ï¼ˆå¦‚æœå­˜åœ¨ä¸”ä¸æ˜¯åˆå§‹ä»‹ç»ï¼‰
            messages = state.get("messages", [])
            
            print(f"ğŸ” DEBUG - Messagesæ•°é‡: {len(messages)}", flush=True)
            for i, msg in enumerate(messages):
                print(f"ğŸ” DEBUG - Message[{i}]: {type(msg).__name__}", flush=True)
            
            # åªæœ‰å½“æœ‰çœŸæ­£çš„å¯¹è¯å†å²ï¼ˆè¶…è¿‡åˆå§‹çš„ç³»ç»Ÿ+ä»‹ç»+ç”¨æˆ·è¾“å…¥ï¼‰æ—¶ï¼Œæ‰æ·»åŠ AIå›å¤
            if len(messages) > 3:  # SystemMessage + AIMessage(intro) + HumanMessage + AIMessage(real_response)
                last_ai_message = None
                # ä»åå¾€å‰æ‰¾ï¼Œè·³è¿‡å¯èƒ½çš„åˆå§‹ä»‹ç»
                for msg in reversed(messages[2:]):  # è·³è¿‡å‰ä¸¤ä¸ªæ¶ˆæ¯ï¼ˆSystemMessage + åˆå§‹ä»‹ç»ï¼‰
                    if isinstance(msg, AIMessage):
                        last_ai_message = msg.content.strip()
                        break
                if last_ai_message:
                    updated_history.append(f"You: {last_ai_message}")
                    print(f"ğŸ” DEBUG - æ·»åŠ AIå†å²: {last_ai_message[:50]}...", flush=True)
            
            # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
            updated_history.append(f"User: {user_message}")
            print(f"ğŸ” DEBUG - æ·»åŠ ç”¨æˆ·æ¶ˆæ¯: {user_message}", flush=True)
            
            # è·å–æœ€è¿‘20è½®çš„å†å²è®°å½•ç”¨äºprompt
            recent_history = updated_history[-20:] if len(updated_history) > 0 else []
            history_context = "\n".join(recent_history) if recent_history else "æ— å¯¹è¯å†å²"
            
            fallback_prompt = f"""## æœ€è¿‘å¯¹è¯å†å²ï¼š
{history_context}

è¯·åŸºäºå¯¹è¯å†å²ï¼Œç”Ÿæˆåˆé€‚çš„CBTç–—æ„ˆå¸ˆå›åº”ã€‚"""
            system_content = PromptTemplates.CBT_THERAPIST_SYSTEM_PROMPT
            
            print("=" * 50, flush=True)
            print("ğŸ” DEBUG - FALLBACK_RESPONSE LLM CALL", flush=True)
            print(f"ğŸ” DEBUG - å†å²è®°å½•æ¡æ•°: {len(updated_history)}", flush=True)
            print("SYSTEM PROMPT:", flush=True)
            print(system_content, flush=True)
            print("USER PROMPT:", flush=True)
            print(fallback_prompt, flush=True)
            print("=" * 50, flush=True)
            
            response = self.llm.invoke([
                SystemMessage(content=system_content),
                HumanMessage(content=fallback_prompt)
            ])
            
            print("RESPONSE:", flush=True)
            print(response.content, flush=True)
            print("=" * 50, flush=True)
            
            # åˆ›å»º AI å›å¤æ¶ˆæ¯å¹¶æ·»åŠ åˆ° messages ä¸­
            ai_response_message = AIMessage(content=response.content)
            
            # æ›´æ–°çŠ¶æ€ï¼ŒåŒ…å«æ›´æ–°åçš„å¯¹è¯å†å²å’Œæ¶ˆæ¯
            updated_state = state.copy()
            updated_state["conversation_history"] = updated_history
            updated_state["messages"] = state.get("messages", []) + [ai_response_message]
            updated_state["final_response"] = response.content
            
            print(f"ğŸ” DEBUG - æ·»åŠ AIå›å¤åˆ°messagesï¼Œæ–°çš„messagesæ•°é‡: {len(updated_state['messages'])}", flush=True)
            
            return response.content, updated_state
            
        except Exception as e:
            fallback_msg = "æŠ±æ­‰ï¼Œæˆ‘ç°åœ¨æ— æ³•å¾ˆå¥½åœ°å›åº”æ‚¨ã€‚è¯·æ‚¨å†è¯•ä¸€æ¬¡ï¼Œæˆ–è€…å‘Šè¯‰æˆ‘æ‚¨æƒ³è¦è¿›è¡Œé—®è¯Šè¿˜æ˜¯æƒ³è¦é—²èŠã€‚"
            return fallback_msg, state