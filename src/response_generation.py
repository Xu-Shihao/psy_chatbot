"""
å›åº”ç”Ÿæˆæ¨¡å—
åŒ…å«è¯„ä¼°æ€»ç»“ç”Ÿæˆå’Œå„ç§æ™ºèƒ½å›åº”åŠŸèƒ½
"""

import json
from typing import Tuple
from langchain.schema import HumanMessage, SystemMessage, AIMessage

from agent import InterviewState
from scid5_knowledge import scid5_kb
from config import config
from prompts import PromptTemplates


class ResponseGenerator:
    """å›åº”ç”Ÿæˆå™¨"""
    
    def __init__(self, openai_client):
        self.openai_client = openai_client
    
    def _build_conversation_messages(self, state: dict, system_prompt: str, new_user_message: str = None) -> list:
        """
        æ„å»ºæ­£ç¡®æ ¼å¼çš„å¯¹è¯æ¶ˆæ¯åˆ—è¡¨: system -> human -> assistant -> human -> assistant...
        
        Args:
            state: å½“å‰çŠ¶æ€
            system_prompt: ç³»ç»Ÿæ¶ˆæ¯å†…å®¹
            new_user_message: æ–°çš„ç”¨æˆ·æ¶ˆæ¯ï¼ˆå¯é€‰ï¼‰
        
        Returns:
            æ ¼å¼æ­£ç¡®çš„æ¶ˆæ¯åˆ—è¡¨
        """
        messages = [SystemMessage(content=system_prompt)]
        
        # è·å–ç°æœ‰çš„å¯¹è¯æ¶ˆæ¯ï¼ˆè·³è¿‡ç³»ç»Ÿæ¶ˆæ¯ï¼‰
        existing_messages = state.get("messages", [])
        
        # è¿‡æ»¤å‡ºäººç±»å’ŒAIæ¶ˆæ¯ï¼Œç¡®ä¿äº¤æ›¿æ ¼å¼
        conversation_messages = []
        for msg in existing_messages:
            if isinstance(msg, (HumanMessage, AIMessage)):
                conversation_messages.append(msg)
        
        # æ·»åŠ ç°æœ‰çš„å¯¹è¯æ¶ˆæ¯
        messages.extend(conversation_messages)
        
        # å¦‚æœæœ‰æ–°çš„ç”¨æˆ·æ¶ˆæ¯ï¼Œæ·»åŠ åˆ°æœ€å
        if new_user_message:
            messages.append(HumanMessage(content=new_user_message))
        
        # è°ƒè¯•è¾“å‡º
        print(f"ğŸ” DEBUG - æ„å»ºçš„æ¶ˆæ¯åºåˆ—:")
        for i, msg in enumerate(messages):
            msg_type = type(msg).__name__
            content_preview = msg.content[:50] if hasattr(msg, 'content') else str(msg)[:50]
            print(f"  [{i}] {msg_type}: {content_preview}...")
        
        return messages
    
    def _format_conversation_context(self, state: dict) -> str:
        """
        æ ¼å¼åŒ–å¯¹è¯å†å²ä¸Šä¸‹æ–‡
        
        Args:
            state: å½“å‰çŠ¶æ€
            
        Returns:
            æ ¼å¼åŒ–çš„å¯¹è¯å†å²å­—ç¬¦ä¸²
        """
        messages = state.get("messages", [])
        if not messages:
            return "æ— å¯¹è¯å†å²"
        
        # è·å–æœ€è¿‘çš„å‡ è½®å¯¹è¯
        recent_messages = messages[-6:] if len(messages) >= 6 else messages
        
        formatted_lines = []
        for msg in recent_messages:
            if isinstance(msg, HumanMessage):
                formatted_lines.append(f"User: {msg.content}")
            elif isinstance(msg, AIMessage):
                formatted_lines.append(f"You: {msg.content}")
            # è·³è¿‡SystemMessage
        
        return "\n".join(formatted_lines) if formatted_lines else "æ— å¯¹è¯å†å²"
    
    def generate_summary(self, state: InterviewState) -> InterviewState:
        """ç”Ÿæˆè¯„ä¼°æ€»ç»“"""
        assessment_summary = scid5_kb.generate_assessment_summary()
        current_messages = state.get("messages", []).copy()
        
        # ä½¿ç”¨ç»Ÿä¸€çš„æ€»ç»“prompt
        summary_system_prompt = PromptTemplates.get_assessment_summary_prompt(
            current_messages, state.get("user_responses", {}), assessment_summary
        )
        
        # æ„å»ºæ­£ç¡®æ ¼å¼çš„æ¶ˆæ¯åˆ—è¡¨
        current_messages = self._build_conversation_messages(
            state, 
            summary_system_prompt, 
            "è¯·åŸºäºæˆ‘ä»¬çš„å®Œæ•´å¯¹è¯å†å²ç”Ÿæˆæœ€ç»ˆè¯„ä¼°æ€»ç»“ã€‚"
        )
        
        print("=" * 50, flush=True)
        print("ğŸ” DEBUG - GENERATE_SUMMARY LLM CALL", flush=True)
        print(f"ğŸ” DEBUG - Messagesæ•°é‡: {len(current_messages)}", flush=True)
        print("=" * 50, flush=True)
        
        # è½¬æ¢ä¸ºOpenAIæ ¼å¼å¹¶è°ƒç”¨API
        from agent import call_openai_api, messages_to_openai_format
        openai_messages = messages_to_openai_format(current_messages)
        response_content = call_openai_api(self.openai_client, openai_messages)
        
        print("RESPONSE:", flush=True)
        print(response_content, flush=True)
        print("=" * 50, flush=True)
        
        detailed_summary = response_content
        
        # æ ¹æ®å·¥ä½œæ¨¡å¼æ·»åŠ ä¸åŒçš„ç»“æŸè¯­
        workflow_mode = getattr(self, 'workflow_mode', 'adaptive')
        if workflow_mode == "structured":
            detailed_summary += PromptTemplates.STRUCTURED_MODE_COMPLETION_MESSAGE
        
        summary_message = AIMessage(content=detailed_summary)
        
        return {
            **state,
            "messages": state["messages"] + [summary_message],
            "summary": detailed_summary,
            "assessment_complete": True,
            "conversation_mode": "assessment_complete",
            "chat_therapist_active": True
        }
    
    def get_next_question_response(self, current_question_id: str, user_response: str, state: dict) -> str:
        """æ ¹æ®å½“å‰é—®é¢˜å’Œç”¨æˆ·å›ç­”ç”Ÿæˆä¸‹ä¸€ä¸ªé—®é¢˜"""
        try:
            # è·å–å½“å‰çš„messages
            current_messages = state.get("messages", []).copy()
            
            # æ ¹æ®debugæ¨¡å¼è°ƒæ•´promptå†…å®¹
            if config.DEBUG:
                # æ„å»ºå¯¹è¯å†å²ä¸Šä¸‹æ–‡
                conversation_context = self._format_conversation_context(state)
                system_prompt = PromptTemplates.get_debug_response_prompt(
                    current_question_id, user_response, conversation_context
                )
            else:
                # è·å–ç”¨æˆ·ä¸»è¯‰å’Œå½“å‰ç—‡çŠ¶
                chief_complaint = state.get("chief_complaint", "")
                current_symptoms = state.get("current_symptoms", [])
                
                # æ„å»ºå¯¹è¯å†å²ä¸Šä¸‹æ–‡
                conversation_context = self._format_conversation_context(state)
                
                system_prompt = PromptTemplates.get_regular_response_prompt(
                    current_question_id, user_response, conversation_context, 
                    chief_complaint, current_symptoms
                )
            
            # æ„å»ºæ­£ç¡®æ ¼å¼çš„æ¶ˆæ¯åˆ—è¡¨
            current_messages = self._build_conversation_messages(
                state, 
                system_prompt, 
                user_response
            )
            
            print("=" * 50, flush=True)
            print("ğŸ” DEBUG - GET_NEXT_QUESTION_RESPONSE LLM CALL", flush=True)
            print(f"ğŸ” DEBUG - Messagesæ•°é‡: {len(current_messages)}", flush=True)
            print("=" * 50, flush=True)
            
            # è½¬æ¢ä¸ºOpenAIæ ¼å¼å¹¶è°ƒç”¨API
            from agent import call_openai_api, messages_to_openai_format
            openai_messages = messages_to_openai_format(current_messages)
            response_content = call_openai_api(self.openai_client, openai_messages)
            
            print("RESPONSE:", flush=True)
            print(response_content, flush=True)
            print("=" * 50, flush=True)
            
            ai_response = response_content
            
            # å¤„ç†debugæ¨¡å¼çš„JSONå›å¤
            if config.DEBUG:
                try:
                    debug_data = json.loads(ai_response)
                    
                    # å­˜å‚¨debugä¿¡æ¯åˆ°çŠ¶æ€ä¸­
                    if "debug_info" not in state:
                        state["debug_info"] = []
                    
                    state["debug_info"].append({
                        "question_id": current_question_id,
                        "user_input": user_response,
                        "analysis": debug_data
                    })
                    
                    # æ£€æŸ¥æ˜¯å¦å®Œæˆè¯„ä¼°
                    if debug_data.get("assessment_complete", False):
                        state["assessment_complete"] = True
                        return self.generate_assessment_summary(state)
                    
                    return debug_data.get("user_response", "è¯·ç»§ç»­åˆ†äº«æ‚¨çš„æ„Ÿå—ã€‚")
                    
                except json.JSONDecodeError:
                    # å¦‚æœJSONè§£æå¤±è´¥ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹å›å¤
                    pass
            
            # å¤„ç†æ­£å¸¸æ¨¡å¼çš„å›å¤
            if "[ASSESSMENT_COMPLETE]" in ai_response:
                state["assessment_complete"] = True
                ai_response = ai_response.replace("[ASSESSMENT_COMPLETE]", "").strip()
                # å…ˆç»™å‡ºå½“å‰å›åº”ï¼Œç„¶åç”Ÿæˆæ€»ç»“
                state["final_response"] = ai_response
                summary_response = self.generate_assessment_summary(state)
                return f"{ai_response}\n\n{summary_response}"
            
            return ai_response
            
        except Exception as e:
            # å¦‚æœLLMè°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨ç®€å•çš„è§„åˆ™
            return self.fallback_question_logic(current_question_id, user_response, state)
    
    def fallback_question_logic(self, current_question_id: str, user_response: str, state: dict) -> str:
        """å¤‡ç”¨çš„ç®€å•è§„åˆ™é€»è¾‘"""
        responses_count = len(state.get("user_responses", {}))
        
        if responses_count >= 3:
            state["assessment_complete"] = True
            return self.generate_simple_summary(state)
        
        # ç®€å•çš„é—®é¢˜åºåˆ—
        if current_question_id == "depression_mood":
            return "è°¢è°¢æ‚¨çš„åˆ†äº«ã€‚æˆ‘æƒ³äº†è§£ä¸€ä¸‹æ‚¨å¹³æ—¶çš„ç”Ÿæ´»çŠ¶æ€ï¼Œæ‚¨æœ€è¿‘è¿˜åƒä»¥å‰ä¸€æ ·å¯¹è‡ªå·±å–œæ¬¢çš„æ´»åŠ¨æ„Ÿåˆ°æœ‰è¶£å’Œå¼€å¿ƒå—ï¼Ÿæ¯”å¦‚çœ‹ç”µå½±ã€å’Œæœ‹å‹èŠå¤©ã€å¬éŸ³ä¹ï¼Œæˆ–è€…å…¶ä»–æ‚¨ä»¥å‰å–œæ¬¢åšçš„äº‹æƒ…ï¼Ÿ"
        elif "depression" in current_question_id:
            return "äº†è§£äº†ã€‚æˆ‘è¿˜æƒ³äº†è§£ä¸€ä¸‹æ‚¨æœ€è¿‘æ˜¯å¦æœ‰ä¸€äº›æ‹…å¿ƒæˆ–è€…ç„¦è™‘çš„æ„Ÿå—ï¼Ÿæ¯”å¦‚å¯¹æœªæ¥çš„æ‹…å¿§ï¼Œæˆ–è€…æ€»æ˜¯æ”¾ä¸ä¸‹å¿ƒæ¥çš„äº‹æƒ…ï¼Ÿ"
        else:
            state["assessment_complete"] = True
            return self.generate_simple_summary(state)
    
    def generate_assessment_summary(self, state: dict) -> str:
        """ä½¿ç”¨LLMç”Ÿæˆè¯„ä¼°æ€»ç»“"""
        try:
            responses = state.get("user_responses", {})
            chief_complaint = state.get("chief_complaint", "")
            current_messages = state.get("messages", []).copy()
            
            system_prompt = f"""
åŸºäºä»¥ä¸‹å¿ƒç†å¥åº·ç­›æŸ¥å¯¹è¯ï¼Œç”Ÿæˆä¸€ä»½ä¸“ä¸šã€æ¸©æš–çš„è¯„ä¼°æ€»ç»“ï¼š

ç”¨æˆ·ä¸»è¯‰ï¼š
{chief_complaint}

ç”¨æˆ·å›ç­”è®°å½•ï¼š
{json.dumps(responses, ensure_ascii=False, indent=2)}

è¯·ç”ŸæˆåŒ…å«ä»¥ä¸‹å†…å®¹çš„æ€»ç»“ï¼š

## ğŸ” è¯„ä¼°æ€»ç»“

### ç”¨æˆ·ä¸»è¯‰
[ç®€è¦é‡è¿°ç”¨æˆ·çš„ä¸»è¦å›°æ‰°å’Œæ±‚åŠ©åŸå› ]

### ä¸»è¦å‘ç°
[åŸºäºå¯¹è¯æ€»ç»“ç”¨æˆ·çš„ä¸»è¦å›°æ‰°å’Œç—‡çŠ¶è¡¨ç°ï¼Œé‡ç‚¹åˆ†æä¸ä¸»è¯‰ç›¸å…³çš„å†…å®¹]

### é£é™©è¯„ä¼°
[è¯„ä¼°å½“å‰çš„å¿ƒç†å¥åº·é£é™©ç­‰çº§]

### ğŸ’¡ é’ˆå¯¹æ€§å»ºè®®
[åŸºäºç”¨æˆ·ä¸»è¯‰å’Œç—‡çŠ¶ï¼Œæä¾›å…·ä½“çš„å»ºè®®å’Œåç»­æ­¥éª¤]

### âš ï¸ é‡è¦è¯´æ˜
- æ­¤è¯„ä¼°ä»…ä¾›å‚è€ƒï¼Œä¸èƒ½æ›¿ä»£ä¸“ä¸šåŒ»ç–—è¯Šæ–­
- å¦‚æœ‰æŒç»­ç—‡çŠ¶ï¼Œå»ºè®®å’¨è¯¢ä¸“ä¸šå¿ƒç†å¥åº·æœåŠ¡
- å¦‚æœ‰ç´§æ€¥æƒ…å†µï¼Œè¯·ç«‹å³æ‹¨æ‰“ï¼š400-161-9995

è¯·ä¿æŒæ¸©æš–ã€æ”¯æŒæ€§çš„è¯­è°ƒï¼Œä½“ç°å¯¹ç”¨æˆ·ä¸»è¯‰çš„ç†è§£å’Œå…³æ³¨ã€‚
"""
            
            # æ„å»ºæ­£ç¡®æ ¼å¼çš„æ¶ˆæ¯åˆ—è¡¨
            current_messages = self._build_conversation_messages(
                state, 
                system_prompt, 
                "è¯·åŸºäºæˆ‘ä»¬çš„å¯¹è¯å†å²ç”Ÿæˆè¯„ä¼°æ€»ç»“ã€‚"
            )
            
            print("=" * 50, flush=True)
            print("ğŸ” DEBUG - GENERATE_ASSESSMENT_SUMMARY LLM CALL", flush=True)
            print(f"ğŸ” DEBUG - Messagesæ•°é‡: {len(current_messages)}", flush=True)
            print("=" * 50, flush=True)
            
            # è½¬æ¢ä¸ºOpenAIæ ¼å¼å¹¶è°ƒç”¨API
            from agent import call_openai_api, messages_to_openai_format
            openai_messages = messages_to_openai_format(current_messages)
            response_content = call_openai_api(self.openai_client, openai_messages)
            
            print("RESPONSE:", flush=True)
            print(response_content, flush=True)
            print("=" * 50, flush=True)
            
            return response_content
            
        except Exception:
            return self.generate_simple_summary(state)
    
    def generate_simple_summary(self, state: dict) -> str:
        """ç”Ÿæˆç®€å•çš„è¯„ä¼°æ€»ç»“"""
        return """## ğŸ” è¯„ä¼°æ€»ç»“

æ„Ÿè°¢æ‚¨å®Œæˆè¿™æ¬¡å¿ƒç†å¥åº·ç­›æŸ¥ã€‚

### ğŸ’¡ å»ºè®®
- å¦‚æœ‰ä»»ä½•å›°æ‰°æˆ–ç—‡çŠ¶æŒç»­ï¼Œå»ºè®®å’¨è¯¢ä¸“ä¸šå¿ƒç†å¥åº·æœåŠ¡
- ä¿æŒè‰¯å¥½çš„ä½œæ¯å’Œç”Ÿæ´»ä¹ æƒ¯
- å¯»æ±‚å®¶äººæœ‹å‹çš„æ”¯æŒ

### âš ï¸ é‡è¦è¯´æ˜
- æ­¤è¯„ä¼°ä»…ä¾›å‚è€ƒï¼Œä¸èƒ½æ›¿ä»£ä¸“ä¸šåŒ»ç–—è¯Šæ–­
- å¦‚æœ‰ç´§æ€¥æƒ…å†µï¼Œè¯·ç«‹å³æ‹¨æ‰“ï¼š400-161-9995

**æ‚¨çš„å¿ƒç†å¥åº·å¾ˆé‡è¦ï¼Œè¯·ä¸è¦çŠ¹è±«å¯»æ±‚ä¸“ä¸šå¸®åŠ©ã€‚**"""
    
    def fallback_response(self, state: dict, user_message: str) -> Tuple[str, dict]:
        """åå¤‡å“åº”æœºåˆ¶"""
        try:
            # æ„å»ºæ­£ç¡®æ ¼å¼çš„æ¶ˆæ¯åˆ—è¡¨
            current_messages = self._build_conversation_messages(
                state, 
                PromptTemplates.CBT_THERAPIST_SYSTEM_PROMPT, 
                user_message
            )
            
            print("=" * 50, flush=True)
            print("ğŸ” DEBUG - FALLBACK_RESPONSE LLM CALL", flush=True)
            print(f"ğŸ” DEBUG - Messagesæ•°é‡: {len(current_messages)}", flush=True)
            print("=" * 50, flush=True)
            
            # è½¬æ¢ä¸ºOpenAIæ ¼å¼å¹¶è°ƒç”¨API
            from agent import call_openai_api, messages_to_openai_format
            openai_messages = messages_to_openai_format(current_messages)
            response_content = call_openai_api(self.openai_client, openai_messages)
            
            print("RESPONSE:", flush=True)
            print(response_content, flush=True)
            print("=" * 50, flush=True)
            
            # åˆ›å»º AI å›å¤æ¶ˆæ¯
            ai_response_message = AIMessage(content=response_content)
            
            # æ›´æ–°çŠ¶æ€
            updated_state = state.copy()
            # å¯¼å…¥å·¥å…·å‡½æ•°å¹¶ä½¿ç”¨OpenAIæ ¼å¼æ›´æ–°conversation_history
            from agent import update_conversation_history_openai
            updated_history = update_conversation_history_openai(state, user_message=user_message)
            updated_state["conversation_history"] = updated_history
            
            # æ›´æ–°messagesï¼šä»current_messagesä¸­ç§»é™¤æœ€åæ·»åŠ çš„ç”¨æˆ·æ¶ˆæ¯ï¼Œç„¶åæ·»åŠ åˆ°åŸå§‹messagesä¸­
            original_messages = state.get("messages", []).copy()
            original_messages.append(HumanMessage(content=user_message))
            original_messages.append(ai_response_message)
            updated_state["messages"] = original_messages
            updated_state["final_response"] = response_content
            
            print(f"ğŸ” DEBUG - æ·»åŠ AIå›å¤åˆ°messagesï¼Œæ–°çš„messagesæ•°é‡: {len(updated_state['messages'])}", flush=True)
            
            return response_content, updated_state
            
        except Exception as e:
            fallback_msg = "æŠ±æ­‰ï¼Œæˆ‘ç°åœ¨æ— æ³•å¾ˆå¥½åœ°å›åº”æ‚¨ã€‚è¯·æ‚¨å†è¯•ä¸€æ¬¡ï¼Œæˆ–è€…å‘Šè¯‰æˆ‘æ‚¨æƒ³è¦è¿›è¡Œé—®è¯Šè¿˜æ˜¯æƒ³è¦é—²èŠã€‚"
            return fallback_msg, state