"""
å›åº”ç”Ÿæˆæ¨¡å—
åŒ…å«è¯„ä¼°æ€»ç»“ç”Ÿæˆå’Œå„ç§æ™ºèƒ½å›åº”åŠŸèƒ½
"""

import json
from typing import Tuple
from langchain.schema import HumanMessage, SystemMessage, AIMessage

from agent_types import InterviewState
from scid5_knowledge import scid5_kb
from config import config


class ResponseGenerator:
    """å›åº”ç”Ÿæˆå™¨"""
    
    def __init__(self, llm):
        self.llm = llm
    
    def generate_summary(self, state: InterviewState) -> InterviewState:
        """ç”Ÿæˆè¯„ä¼°æ€»ç»“"""
        assessment_summary = scid5_kb.generate_assessment_summary()
        
        # ä½¿ç”¨LLMç”Ÿæˆæ›´è¯¦ç»†å’Œä¸ªæ€§åŒ–çš„æ€»ç»“
        prompt = f"""åŸºäºä»¥ä¸‹å®Œæ•´çš„å¿ƒç†å¥åº·ç­›æŸ¥å¯¹è¯ï¼Œç”Ÿæˆä¸€ä»½ä¸“ä¸šã€ä¸ªæ€§åŒ–çš„è¯„ä¼°æ€»ç»“ï¼š

å¯¹è¯å†å²ï¼š
{chr(10).join(state["conversation_history"])}

ç”¨æˆ·å›ç­”è®°å½•ï¼š
{json.dumps(state["user_responses"], ensure_ascii=False, indent=2)}

ç»“æ„åŒ–è¯„ä¼°æ€»ç»“ï¼š
{assessment_summary}

è¯·ç”Ÿæˆä¸€ä»½åŒ…å«ä»¥ä¸‹å†…å®¹çš„ä¸ªæ€§åŒ–æ€»ç»“ï¼š

## ğŸ” è¯„ä¼°æ€»ç»“

### ä¸»è¦å‘ç°
- åŸºäºå¯¹è¯å†…å®¹æ€»ç»“ç”¨æˆ·çš„ä¸»è¦å›°æ‰°å’Œç—‡çŠ¶è¡¨ç°
- è¯†åˆ«çš„æƒ…æ„Ÿæ¨¡å¼å’Œè¡Œä¸ºç‰¹å¾

### é£é™©è¯„ä¼°
- å½“å‰çš„å¿ƒç†å¥åº·é£é™©ç­‰çº§
- éœ€è¦ç‰¹åˆ«å…³æ³¨çš„æ–¹é¢

### ğŸ’¡ å»ºè®®å’Œåç»­æ­¥éª¤
- å…·ä½“çš„ä¸‹ä¸€æ­¥è¡ŒåŠ¨å»ºè®®
- å¯èƒ½æœ‰å¸®åŠ©çš„èµ„æºå’Œæ”¯æŒ

### âš ï¸ é‡è¦è¯´æ˜
- å¼ºè°ƒè¿™æ˜¯ç­›æŸ¥å·¥å…·ï¼Œä¸èƒ½æ›¿ä»£ä¸“ä¸šè¯Šæ–­
- é¼“åŠ±å¯»æ±‚ä¸“ä¸šå¸®åŠ©

è¯·ä¿æŒæ¸©æš–ã€æ”¯æŒæ€§çš„è¯­è°ƒï¼Œé¿å…ä½¿ç”¨å¯èƒ½å¼•èµ·ç„¦è™‘çš„åŒ»å­¦æœ¯è¯­ã€‚"""
        
        print("=" * 50)
        print("ğŸ” DEBUG - GENERATE_SUMMARY LLM CALL")
        print("PROMPT:")
        print(prompt)
        print("=" * 50)
        
        response = self.llm.invoke([HumanMessage(content=prompt)])
        
        print("RESPONSE:")
        print(response.content)
        print("=" * 50)
        
        detailed_summary = response.content
        
        # æ ¹æ®å·¥ä½œæ¨¡å¼æ·»åŠ ä¸åŒçš„ç»“æŸè¯­
        workflow_mode = getattr(self, 'workflow_mode', 'adaptive')
        if workflow_mode == "structured":
            detailed_summary += """
            
---

ğŸŒŸ **é—®è¯Šè¯„ä¼°å·²å®Œæˆï¼**

ç°åœ¨æˆ‘ä»¬å¯ä»¥è¿›å…¥æ›´è½»æ¾çš„äº¤æµç¯èŠ‚ã€‚å¦‚æœæ‚¨æœ‰ä»»ä½•æƒ³èŠçš„è¯é¢˜ï¼Œæˆ–è€…éœ€è¦æƒ…ç»ªæ”¯æŒå’Œå¿ƒç†å»ºè®®ï¼Œæˆ‘å¾ˆä¹æ„ä»¥CBTç–—æ„ˆå¸ˆçš„èº«ä»½ç»§ç»­é™ªä¼´æ‚¨ã€‚

æ‚¨å¯ä»¥ï¼š
- åˆ†äº«æ‚¨ç°åœ¨çš„æ„Ÿå—
- èŠèŠæ—¥å¸¸ç”Ÿæ´»ä¸­çš„äº‹æƒ…  
- å¯»æ±‚åº”å¯¹å›°éš¾çš„å»ºè®®
- æˆ–è€…ä»»ä½•æ‚¨æƒ³è°ˆè®ºçš„è¯é¢˜

æˆ‘åœ¨è¿™é‡Œé™ªä¼´æ‚¨ ğŸ’•
"""
        
        summary_message = AIMessage(content=detailed_summary)
        
        return {
            **state,
            "messages": state["messages"] + [summary_message],
            "summary": detailed_summary,
            "assessment_complete": True,
            "conversation_mode": "assessment_complete",
            "chat_therapist_active": True
        }
    
    def get_next_question_response(self, current_question_id: str, user_response: str, state: dict) -> str:
        """æ ¹æ®å½“å‰é—®é¢˜å’Œç”¨æˆ·å›ç­”ç”Ÿæˆä¸‹ä¸€ä¸ªé—®é¢˜"""
        try:
            # ä½¿ç”¨LLMç”Ÿæˆä¸ªæ€§åŒ–çš„å›åº”å’Œä¸‹ä¸€ä¸ªé—®é¢˜
            conversation_context = "\n".join(state.get("conversation_history", []))[-500:]  # é™åˆ¶é•¿åº¦
            
            # æ ¹æ®debugæ¨¡å¼è°ƒæ•´prompt
            if config.DEBUG:
                prompt = f"""
ä½ çš„åå­—å«é©¬å¿ƒå®ï¼Œæ˜¯ä¸€ä½ä¸“ä¸šçš„å¿ƒç†å’¨è¯¢å¸ˆï¼Œæ­£åœ¨è¿›è¡ŒSCID-5ç»“æ„åŒ–ä¸´åºŠè®¿è°ˆæ¥è¿›è¡Œé¢„é—®è¯Šå’Œå¿ƒç†ç–å¯¼ã€‚

å½“å‰é—®é¢˜ID: {current_question_id}
ç”¨æˆ·å›ç­”: {user_response}
å¯¹è¯å†å²: {conversation_context}

è¯·ä»¥é©¬å¿ƒå®çš„èº«ä»½æ ¹æ®ç”¨æˆ·çš„å›ç­”è¿›è¡Œåˆ†æå’Œå›åº”ï¼š

1. **æƒ…æ„Ÿç†è§£**ï¼šè¯†åˆ«ç”¨æˆ·çš„æƒ…æ„ŸçŠ¶æ€
2. **ç—‡çŠ¶è¯„ä¼°**ï¼šåˆ†ææ˜¯å¦æœ‰å¿ƒç†å¥åº·ç—‡çŠ¶æŒ‡æ ‡
3. **ä¸‹ä¸€æ­¥å†³ç­–**ï¼šé€‰æ‹©æœ€åˆé€‚çš„åç»­é—®é¢˜

å¯é€‰çš„é—®é¢˜é¢†åŸŸï¼š
- æŠ‘éƒç—‡çŠ¶ï¼ˆæƒ…ç»ªä½è½ã€å…´è¶£ç¼ºå¤±ã€ç¡çœ ã€é£Ÿæ¬²ã€ç–²åŠ³ç­‰ï¼‰
- ç„¦è™‘ç—‡çŠ¶ï¼ˆè¿‡åº¦æ‹…å¿ƒã€ææ…Œã€å›é¿è¡Œä¸ºç­‰ï¼‰
- ç²¾ç¥ç—…æ€§ç—‡çŠ¶ï¼ˆå¹»è§‰ã€å¦„æƒ³ç­‰ï¼‰
- ç‰©è´¨ä½¿ç”¨é—®é¢˜
- åˆ›ä¼¤ç»å†

è¯·ä»¥JSONæ ¼å¼å›å¤ï¼Œç”¨äºdebugåˆ†æï¼š
{{
    "emotional_analysis": "ç”¨æˆ·æƒ…æ„ŸçŠ¶æ€åˆ†æ",
    "symptom_indicators": ["å‘ç°çš„ç—‡çŠ¶æŒ‡æ ‡"],
    "risk_assessment": "é£é™©è¯„ä¼°",
    "next_question_rationale": "é€‰æ‹©ä¸‹ä¸€ä¸ªé—®é¢˜çš„ç†ç”±",
    "user_response": "é©¬å¿ƒå®çš„è‡ªç„¶å…±æƒ…å›åº”å’Œä¸‹ä¸€ä¸ªé—®é¢˜",
    "assessment_complete": false
}}

å¦‚æœè¯„ä¼°åº”è¯¥ç»“æŸï¼Œè¯·è®¾ç½®"assessment_complete": trueã€‚
"""
            else:
                # è·å–ç”¨æˆ·ä¸»è¯‰
                chief_complaint = state.get("chief_complaint", "")
                
                prompt = f"""
ä½ çš„åå­—å«é©¬å¿ƒå®ï¼Œæ˜¯ä¸€ä½æ¸©æš–ã€ä¸“ä¸šçš„å¿ƒç†å’¨è¯¢å¸ˆï¼Œæ­£åœ¨ä¸æ¥è®¿è€…è¿›è¡Œæ¸©å’Œçš„å¯¹è¯äº†è§£ï¼Œè¿›è¡Œé¢„é—®è¯Šå’Œå¿ƒç†ç–å¯¼ã€‚

ç”¨æˆ·çš„ä¸»è¯‰ï¼š"{chief_complaint}"
ç”¨æˆ·åˆšæ‰è¯´ï¼š"{user_response}"

è¯·ä»¥é©¬å¿ƒå®çš„èº«ä»½åƒçœŸæ­£çš„å¿ƒç†å’¨è¯¢å¸ˆä¸€æ ·å›åº”ï¼š
1. é¦–å…ˆè¡¨è¾¾ç†è§£å’Œå…±æƒ…
2. åŸºäºç”¨æˆ·çš„ä¸»è¯‰å’Œå½“å‰å›ç­”ï¼Œè‡ªç„¶åœ°å¼•å‡ºä¸‹ä¸€ä¸ªç›¸å…³é—®é¢˜
3. ä¿æŒå¯¹è¯çš„è‡ªç„¶æµç•…ï¼Œä¸è¦æ˜¾å¾—è¿‡äºç»“æ„åŒ–
4. å›´ç»•ç”¨æˆ·çš„ä¸»è¦å›°æ‰°è¿›è¡Œæ·±å…¥äº†è§£

å¯¹è¯é£æ ¼è¦æ±‚ï¼š
- æ¸©æš–ã€ç†è§£ã€è‡ªç„¶
- åƒæœ‹å‹èˆ¬çš„ä¸“ä¸šå…³æ€€
- é¿å…æ˜æ˜¾çš„"é—®å·å¼"æé—®
- æ ¹æ®ç”¨æˆ·çš„ä¸»è¯‰å’Œå›ç­”å†…å®¹è‡ªç„¶å»¶å±•
- ä½“ç°é©¬å¿ƒå®ä¸“ä¸šè€Œæ¸©æš–çš„ä¸ªäººé£æ ¼

é‡ç‚¹å…³æ³¨é¢†åŸŸï¼ˆä½†è¦ç»“åˆç”¨æˆ·ä¸»è¯‰è¿›è¡Œä¸ªæ€§åŒ–è¯¢é—®ï¼‰ï¼š
- ä¸ä¸»è¯‰ç›¸å…³çš„å…·ä½“ç—‡çŠ¶å’Œè¡¨ç°
- é—®é¢˜çš„æŒç»­æ—¶é—´å’Œå‘å±•è¿‡ç¨‹
- å¯¹æ—¥å¸¸ç”Ÿæ´»çš„å…·ä½“å½±å“
- æƒ…ç»ªçŠ¶æ€å’Œå¿ƒå¢ƒå˜åŒ–
- ç¡çœ ã€é£Ÿæ¬²ã€ç²¾åŠ›ç­‰ç”Ÿç†æ–¹é¢
- ç¤¾äº¤å’Œå·¥ä½œæƒ…å†µ
- æ—¢å¾€çš„åº”å¯¹æ–¹å¼å’Œæ±‚åŠ©ç»å†

è¯·ç›´æ¥ç»™å‡ºè‡ªç„¶çš„å¯¹è¯å›åº”ï¼Œä¸éœ€è¦ä»»ä½•æ ¼å¼æ ‡è®°ã€‚å¦‚æœè§‰å¾—å·²ç»äº†è§£è¶³å¤Ÿä¿¡æ¯ï¼Œå¯ä»¥å¼€å§‹æ€»ç»“ã€‚

å¦‚æœéœ€è¦ç»“æŸè¯„ä¼°ï¼Œè¯·åœ¨å›åº”æœ€ååŠ ä¸Š"[ASSESSMENT_COMPLETE]"ã€‚
"""
            
            print("=" * 50)
            print("ğŸ” DEBUG - GET_NEXT_QUESTION_RESPONSE LLM CALL")
            print("PROMPT:")
            print(prompt)
            print("=" * 50)
            
            response = self.llm.invoke([HumanMessage(content=prompt)])
            
            print("RESPONSE:")
            print(response.content)
            print("=" * 50)
            
            ai_response = response.content
            
            # å¤„ç†debugæ¨¡å¼çš„JSONå›å¤
            if config.DEBUG:
                try:
                    debug_data = json.loads(ai_response)
                    
                    # å­˜å‚¨debugä¿¡æ¯åˆ°çŠ¶æ€ä¸­
                    if "debug_info" not in state:
                        state["debug_info"] = []
                    
                    state["debug_info"].append({
                        "question_id": current_question_id,
                        "user_input": user_response,
                        "analysis": debug_data
                    })
                    
                    # æ£€æŸ¥æ˜¯å¦å®Œæˆè¯„ä¼°
                    if debug_data.get("assessment_complete", False):
                        state["assessment_complete"] = True
                        return self.generate_assessment_summary(state)
                    
                    return debug_data.get("user_response", "è¯·ç»§ç»­åˆ†äº«æ‚¨çš„æ„Ÿå—ã€‚")
                    
                except json.JSONDecodeError:
                    # å¦‚æœJSONè§£æå¤±è´¥ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹å›å¤
                    pass
            
            # å¤„ç†æ­£å¸¸æ¨¡å¼çš„å›å¤
            if "[ASSESSMENT_COMPLETE]" in ai_response:
                state["assessment_complete"] = True
                ai_response = ai_response.replace("[ASSESSMENT_COMPLETE]", "").strip()
                # å…ˆç»™å‡ºå½“å‰å›åº”ï¼Œç„¶åç”Ÿæˆæ€»ç»“
                state["final_response"] = ai_response
                summary_response = self.generate_assessment_summary(state)
                return f"{ai_response}\n\n{summary_response}"
            
            return ai_response
            
        except Exception as e:
            # å¦‚æœLLMè°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨ç®€å•çš„è§„åˆ™
            return self.fallback_question_logic(current_question_id, user_response, state)
    
    def fallback_question_logic(self, current_question_id: str, user_response: str, state: dict) -> str:
        """å¤‡ç”¨çš„ç®€å•è§„åˆ™é€»è¾‘"""
        responses_count = len(state.get("user_responses", {}))
        
        if responses_count >= 3:
            state["assessment_complete"] = True
            return self.generate_simple_summary(state)
        
        # ç®€å•çš„é—®é¢˜åºåˆ—
        if current_question_id == "depression_mood":
            return "è°¢è°¢æ‚¨çš„åˆ†äº«ã€‚æˆ‘æƒ³äº†è§£ä¸€ä¸‹æ‚¨å¹³æ—¶çš„ç”Ÿæ´»çŠ¶æ€ï¼Œæ‚¨æœ€è¿‘è¿˜åƒä»¥å‰ä¸€æ ·å¯¹è‡ªå·±å–œæ¬¢çš„æ´»åŠ¨æ„Ÿåˆ°æœ‰è¶£å’Œå¼€å¿ƒå—ï¼Ÿæ¯”å¦‚çœ‹ç”µå½±ã€å’Œæœ‹å‹èŠå¤©ã€å¬éŸ³ä¹ï¼Œæˆ–è€…å…¶ä»–æ‚¨ä»¥å‰å–œæ¬¢åšçš„äº‹æƒ…ï¼Ÿ"
        elif "depression" in current_question_id:
            return "äº†è§£äº†ã€‚æˆ‘è¿˜æƒ³äº†è§£ä¸€ä¸‹æ‚¨æœ€è¿‘æ˜¯å¦æœ‰ä¸€äº›æ‹…å¿ƒæˆ–è€…ç„¦è™‘çš„æ„Ÿå—ï¼Ÿæ¯”å¦‚å¯¹æœªæ¥çš„æ‹…å¿§ï¼Œæˆ–è€…æ€»æ˜¯æ”¾ä¸ä¸‹å¿ƒæ¥çš„äº‹æƒ…ï¼Ÿ"
        else:
            state["assessment_complete"] = True
            return self.generate_simple_summary(state)
    
    def generate_assessment_summary(self, state: dict) -> str:
        """ä½¿ç”¨LLMç”Ÿæˆè¯„ä¼°æ€»ç»“"""
        try:
            responses = state.get("user_responses", {})
            conversation = "\n".join(state.get("conversation_history", []))
            chief_complaint = state.get("chief_complaint", "")
            
            prompt = f"""
åŸºäºä»¥ä¸‹å¿ƒç†å¥åº·ç­›æŸ¥å¯¹è¯ï¼Œç”Ÿæˆä¸€ä»½ä¸“ä¸šã€æ¸©æš–çš„è¯„ä¼°æ€»ç»“ï¼š

ç”¨æˆ·ä¸»è¯‰ï¼š
{chief_complaint}

ç”¨æˆ·å›ç­”è®°å½•ï¼š
{json.dumps(responses, ensure_ascii=False, indent=2)}

å¯¹è¯å†å²ï¼š
{conversation}

        è¯·ç”ŸæˆåŒ…å«ä»¥ä¸‹å†…å®¹çš„æ€»ç»“ï¼š

        ## ğŸ” è¯„ä¼°æ€»ç»“

        ### ç”¨æˆ·ä¸»è¯‰
        [ç®€è¦é‡è¿°ç”¨æˆ·çš„ä¸»è¦å›°æ‰°å’Œæ±‚åŠ©åŸå› ]

        ### ä¸»è¦å‘ç°
        [åŸºäºå¯¹è¯æ€»ç»“ç”¨æˆ·çš„ä¸»è¦å›°æ‰°å’Œç—‡çŠ¶è¡¨ç°ï¼Œé‡ç‚¹åˆ†æä¸ä¸»è¯‰ç›¸å…³çš„å†…å®¹]

        ### é£é™©è¯„ä¼°
        [è¯„ä¼°å½“å‰çš„å¿ƒç†å¥åº·é£é™©ç­‰çº§]

        ### ğŸ’¡ é’ˆå¯¹æ€§å»ºè®®
        [åŸºäºç”¨æˆ·ä¸»è¯‰å’Œç—‡çŠ¶ï¼Œæä¾›å…·ä½“çš„å»ºè®®å’Œåç»­æ­¥éª¤]

        ### âš ï¸ é‡è¦è¯´æ˜
        - æ­¤è¯„ä¼°ä»…ä¾›å‚è€ƒï¼Œä¸èƒ½æ›¿ä»£ä¸“ä¸šåŒ»ç–—è¯Šæ–­
        - å¦‚æœ‰æŒç»­ç—‡çŠ¶ï¼Œå»ºè®®å’¨è¯¢ä¸“ä¸šå¿ƒç†å¥åº·æœåŠ¡
        - å¦‚æœ‰ç´§æ€¥æƒ…å†µï¼Œè¯·ç«‹å³æ‹¨æ‰“ï¼š400-161-9995

        è¯·ä¿æŒæ¸©æš–ã€æ”¯æŒæ€§çš„è¯­è°ƒï¼Œä½“ç°å¯¹ç”¨æˆ·ä¸»è¯‰çš„ç†è§£å’Œå…³æ³¨ã€‚
"""
            
            print("=" * 50)
            print("ğŸ” DEBUG - GENERATE_ASSESSMENT_SUMMARY LLM CALL")
            print("PROMPT:")
            print(prompt)
            print("=" * 50)
            
            response = self.llm.invoke([HumanMessage(content=prompt)])
            
            print("RESPONSE:")
            print(response.content)
            print("=" * 50)
            
            return response.content
            
        except Exception:
            return self.generate_simple_summary(state)
    
    def generate_simple_summary(self, state: dict) -> str:
        """ç”Ÿæˆç®€å•çš„è¯„ä¼°æ€»ç»“"""
        return """## ğŸ” è¯„ä¼°æ€»ç»“

æ„Ÿè°¢æ‚¨å®Œæˆè¿™æ¬¡å¿ƒç†å¥åº·ç­›æŸ¥ã€‚

### ğŸ’¡ å»ºè®®
- å¦‚æœ‰ä»»ä½•å›°æ‰°æˆ–ç—‡çŠ¶æŒç»­ï¼Œå»ºè®®å’¨è¯¢ä¸“ä¸šå¿ƒç†å¥åº·æœåŠ¡
- ä¿æŒè‰¯å¥½çš„ä½œæ¯å’Œç”Ÿæ´»ä¹ æƒ¯
- å¯»æ±‚å®¶äººæœ‹å‹çš„æ”¯æŒ

### âš ï¸ é‡è¦è¯´æ˜
- æ­¤è¯„ä¼°ä»…ä¾›å‚è€ƒï¼Œä¸èƒ½æ›¿ä»£ä¸“ä¸šåŒ»ç–—è¯Šæ–­
- å¦‚æœ‰ç´§æ€¥æƒ…å†µï¼Œè¯·ç«‹å³æ‹¨æ‰“ï¼š400-161-9995

**æ‚¨çš„å¿ƒç†å¥åº·å¾ˆé‡è¦ï¼Œè¯·ä¸è¦çŠ¹è±«å¯»æ±‚ä¸“ä¸šå¸®åŠ©ã€‚**"""
    
    def fallback_response(self, state: dict, user_message: str) -> Tuple[str, dict]:
        """åå¤‡å“åº”æœºåˆ¶"""
        try:
            fallback_prompt = f"ç”¨æˆ·è¯´ï¼š{user_message}"
            system_content = "ä½ æ˜¯çµæºªæ™ºä¼´ï¼Œä¸€ä½ä¸“ä¸šçš„å¿ƒç†å’¨è¯¢å¸ˆã€‚"
            
            print("=" * 50)
            print("ğŸ” DEBUG - FALLBACK_RESPONSE LLM CALL")
            print("SYSTEM PROMPT:")
            print(system_content)
            print("USER PROMPT:")
            print(fallback_prompt)
            print("=" * 50)
            
            response = self.llm.invoke([
                SystemMessage(content=system_content),
                HumanMessage(content=fallback_prompt)
            ])
            
            print("RESPONSE:")
            print(response.content)
            print("=" * 50)
            
            state["final_response"] = response.content
            return response.content, state
            
        except Exception as e:
            fallback_msg = "æŠ±æ­‰ï¼Œæˆ‘ç°åœ¨æ— æ³•å¾ˆå¥½åœ°å›åº”æ‚¨ã€‚è¯·æ‚¨å†è¯•ä¸€æ¬¡ï¼Œæˆ–è€…å‘Šè¯‰æˆ‘æ‚¨æƒ³è¦è¿›è¡Œé—®è¯Šè¿˜æ˜¯æƒ³è¦é—²èŠã€‚"
            return fallback_msg, state